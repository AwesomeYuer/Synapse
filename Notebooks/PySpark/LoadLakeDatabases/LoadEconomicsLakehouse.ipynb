{"cells":[{"cell_type":"markdown","metadata":{"azdata_cell_guid":"06f9bc26-79a6-45ce-b4a9-b906f4870bc1","nteract":{"transient":{"deleting":false}}},"source":["## US Labor Force Statistics\n","\n","Labor Force Statistics labor force, labor force participation rates, and the civilian noninstitutional population by age, gender, race, and ethnic groups. in the United States.\n","\n","This dataset is sourced from Current Employment Statistics - CES (National) data published by US Bureau of Labor Statistics (BLS). Review Linking and Copyright Information and Important Web Site Notices for the terms and conditions related to the use this dataset."]},{"cell_type":"markdown","metadata":{"azdata_cell_guid":"99744c9b-3fd7-4564-a94e-3d10f72d52c7","nteract":{"transient":{"deleting":false}}},"source":["# Labor and economics"]},{"cell_type":"code","execution_count":1,"metadata":{"azdata_cell_guid":"3636872f-f144-4275-988c-419df683700c","jupyter":{"outputs_hidden":false,"source_hidden":false},"language":"python","nteract":{"transient":{"deleting":false}}},"outputs":[{"data":{"application/vnd.livy.statement-meta+json":{"execution_finish_time":"2022-10-25T08:36:08.0318531Z","execution_start_time":"2022-10-25T08:34:46.9421671Z","livy_statement_state":"available","queued_time":"2022-10-25T08:34:19.6664309Z","session_id":"d7adf307-1e22-40aa-8549-f8efa4c9564e","session_start_time":"2022-10-25T08:34:19.6698776Z","spark_jobs":{"jobs":[{"completionTime":"2022-10-25T08:36:02.912GMT","dataRead":4772,"dataWritten":0,"description":"Delta: Job group for statement 2:\n# Azure storage access info\nblob_account_name = \"azureopendatastorage\"\nblob_container_name = \"laborstatisticscontainer\"\nblob_relative_path = \"lfs/\"\nblob_sas_token = r\"\"\n\n# Allow SPARK to read from Blob remotely\nwasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\nspark.conf.set(\n  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n  blob_sas_token)\nprint('Remote blob path: ' + wasbs_path)\n\ndf = spark.read.parquet(wasbs_path)\ndf.write.format(\"delta\").saveAsTable(\"us_labor_force_statistics\"): Compute snapshot for version: 0","jobGroup":"2","jobId":4,"killedTasksSummary":{},"name":"$anonfun$recordDeltaOperation$5 at SynapseLoggingShim.scala:86","numActiveStages":-2,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":2,"numSkippedTasks":51,"numTasks":52,"rowCount":50,"stageIds":[5,6,7],"status":"SUCCEEDED","submissionTime":"2022-10-25T08:36:02.478GMT"},{"completionTime":"2022-10-25T08:36:02.407GMT","dataRead":9506,"dataWritten":4772,"description":"Delta: Job group for statement 2:\n# Azure storage access info\nblob_account_name = \"azureopendatastorage\"\nblob_container_name = \"laborstatisticscontainer\"\nblob_relative_path = \"lfs/\"\nblob_sas_token = r\"\"\n\n# Allow SPARK to read from Blob remotely\nwasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\nspark.conf.set(\n  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n  blob_sas_token)\nprint('Remote blob path: ' + wasbs_path)\n\ndf = spark.read.parquet(wasbs_path)\ndf.write.format(\"delta\").saveAsTable(\"us_labor_force_statistics\"): Compute snapshot for version: 0","jobGroup":"2","jobId":3,"killedTasksSummary":{},"name":"$anonfun$recordDeltaOperation$5 at SynapseLoggingShim.scala:86","numActiveStages":-1,"numActiveTasks":0,"numCompletedIndices":50,"numCompletedStages":1,"numCompletedTasks":50,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":1,"numTasks":51,"rowCount":61,"stageIds":[3,4],"status":"SUCCEEDED","submissionTime":"2022-10-25T08:35:59.746GMT"},{"completionTime":"2022-10-25T08:35:58.499GMT","dataRead":20242,"dataWritten":9506,"description":"Delta: Job group for statement 2:\n# Azure storage access info\nblob_account_name = \"azureopendatastorage\"\nblob_container_name = \"laborstatisticscontainer\"\nblob_relative_path = \"lfs/\"\nblob_sas_token = r\"\"\n\n# Allow SPARK to read from Blob remotely\nwasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\nspark.conf.set(\n  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n  blob_sas_token)\nprint('Remote blob path: ' + wasbs_path)\n\ndf = spark.read.parquet(wasbs_path)\ndf.write.format(\"delta\").saveAsTable(\"us_labor_force_statistics\"): Compute snapshot for version: 0","jobGroup":"2","jobId":2,"killedTasksSummary":{},"name":"$anonfun$recordDeltaOperation$5 at SynapseLoggingShim.scala:86","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":1,"rowCount":22,"stageIds":[2],"status":"SUCCEEDED","submissionTime":"2022-10-25T08:35:57.218GMT"},{"completionTime":"2022-10-25T08:35:51.397GMT","dataRead":101347488,"dataWritten":19013003,"description":"Job group for statement 2:\n# Azure storage access info\nblob_account_name = \"azureopendatastorage\"\nblob_container_name = \"laborstatisticscontainer\"\nblob_relative_path = \"lfs/\"\nblob_sas_token = r\"\"\n\n# Allow SPARK to read from Blob remotely\nwasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\nspark.conf.set(\n  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n  blob_sas_token)\nprint('Remote blob path: ' + wasbs_path)\n\ndf = spark.read.parquet(wasbs_path)\ndf.write.format(\"delta\").saveAsTable(\"us_labor_force_statistics\")","jobGroup":"2","jobId":1,"killedTasksSummary":{},"name":"$anonfun$recordDeltaOperation$5 at SynapseLoggingShim.scala:86","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":10,"numCompletedStages":1,"numCompletedTasks":10,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":10,"rowCount":13165452,"stageIds":[1],"status":"SUCCEEDED","submissionTime":"2022-10-25T08:35:18.676GMT"},{"completionTime":"2022-10-25T08:34:54.143GMT","dataRead":0,"dataWritten":0,"description":"Job group for statement 2:\n# Azure storage access info\nblob_account_name = \"azureopendatastorage\"\nblob_container_name = \"laborstatisticscontainer\"\nblob_relative_path = \"lfs/\"\nblob_sas_token = r\"\"\n\n# Allow SPARK to read from Blob remotely\nwasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\nspark.conf.set(\n  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n  blob_sas_token)\nprint('Remote blob path: ' + wasbs_path)\n\ndf = spark.read.parquet(wasbs_path)\ndf.write.format(\"delta\").saveAsTable(\"us_labor_force_statistics\")","jobGroup":"2","jobId":0,"killedTasksSummary":{},"name":"parquet at NativeMethodAccessorImpl.java:0","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":1,"rowCount":0,"stageIds":[0],"status":"SUCCEEDED","submissionTime":"2022-10-25T08:34:48.200GMT"}],"limit":20,"numbers":{"FAILED":0,"RUNNING":0,"SUCCEEDED":5,"UNKNOWN":0},"rule":"ALL_DESC"},"spark_pool":null,"state":"finished","statement_id":2},"text/plain":["StatementMeta(, d7adf307-1e22-40aa-8549-f8efa4c9564e, 2, Finished, Available)"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Remote blob path: wasbs://laborstatisticscontainer@azureopendatastorage.blob.core.windows.net/lfs/\n"]}],"source":["# Azure storage access info\n","blob_account_name = \"azureopendatastorage\"\n","blob_container_name = \"laborstatisticscontainer\"\n","blob_relative_path = \"lfs/\"\n","blob_sas_token = r\"\"\n","\n","# Allow SPARK to read from Blob remotely\n","wasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\n","spark.conf.set(\n","  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n","  blob_sas_token)\n","print('Remote blob path: ' + wasbs_path)\n","\n","df = spark.read.parquet(wasbs_path)\n","df.write.format(\"delta\").saveAsTable(\"us_labor_force_statistics\")"]},{"cell_type":"markdown","metadata":{"azdata_cell_guid":"f0ffa504-114c-420b-bdb5-b289206a653c","nteract":{"transient":{"deleting":false}}},"source":["**US National Employment Hours and Earnings**\n","\n","The Current Employment Statistics (CES) program produces detailed industry estimates of nonfarm employment, hours, and earnings of workers on payrolls in the United States."]},{"cell_type":"code","execution_count":2,"metadata":{"azdata_cell_guid":"cf3da585-2be1-4b6e-859b-45989c82e6e8","jupyter":{"outputs_hidden":false,"source_hidden":false},"language":"python","nteract":{"transient":{"deleting":false}}},"outputs":[{"data":{"application/vnd.livy.statement-meta+json":{"execution_finish_time":"2022-10-25T08:36:36.914768Z","execution_start_time":"2022-10-25T08:36:08.1200232Z","livy_statement_state":"available","queued_time":"2022-10-25T08:34:19.6692228Z","session_id":"d7adf307-1e22-40aa-8549-f8efa4c9564e","session_start_time":null,"spark_jobs":{"jobs":[{"completionTime":"2022-10-25T08:36:31.008GMT","dataRead":4552,"dataWritten":0,"description":"Delta: Job group for statement 3:\n# Azure storage access info\nblob_account_name = \"azureopendatastorage\"\nblob_container_name = \"laborstatisticscontainer\"\nblob_relative_path = \"ehe_national/\"\nblob_sas_token = r\"\"\n\n# Allow SPARK to read from Blob remotely\nwasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\nspark.conf.set(\n  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n  blob_sas_token)\nprint('Remote blob path: ' + wasbs_path)\n\n# SPARK read parquet, note that it won't load any data yet by now\ndf = spark.read.parquet(wasbs_path)\ndf.write.format(\"delta\").saveAsTable(\"us_employment_hours_earnings_national\"): Compute snapshot for version: 0","jobGroup":"3","jobId":9,"killedTasksSummary":{},"name":"$anonfun$recordDeltaOperation$5 at SynapseLoggingShim.scala:86","numActiveStages":-2,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":2,"numSkippedTasks":51,"numTasks":52,"rowCount":50,"stageIds":[15,13,14],"status":"SUCCEEDED","submissionTime":"2022-10-25T08:36:30.917GMT"},{"completionTime":"2022-10-25T08:36:30.878GMT","dataRead":8205,"dataWritten":4552,"description":"Delta: Job group for statement 3:\n# Azure storage access info\nblob_account_name = \"azureopendatastorage\"\nblob_container_name = \"laborstatisticscontainer\"\nblob_relative_path = \"ehe_national/\"\nblob_sas_token = r\"\"\n\n# Allow SPARK to read from Blob remotely\nwasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\nspark.conf.set(\n  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n  blob_sas_token)\nprint('Remote blob path: ' + wasbs_path)\n\n# SPARK read parquet, note that it won't load any data yet by now\ndf = spark.read.parquet(wasbs_path)\ndf.write.format(\"delta\").saveAsTable(\"us_employment_hours_earnings_national\"): Compute snapshot for version: 0","jobGroup":"3","jobId":8,"killedTasksSummary":{},"name":"$anonfun$recordDeltaOperation$5 at SynapseLoggingShim.scala:86","numActiveStages":-1,"numActiveTasks":0,"numCompletedIndices":50,"numCompletedStages":1,"numCompletedTasks":50,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":1,"numTasks":51,"rowCount":61,"stageIds":[12,11],"status":"SUCCEEDED","submissionTime":"2022-10-25T08:36:30.259GMT"},{"completionTime":"2022-10-25T08:36:30.006GMT","dataRead":12148,"dataWritten":8205,"description":"Delta: Job group for statement 3:\n# Azure storage access info\nblob_account_name = \"azureopendatastorage\"\nblob_container_name = \"laborstatisticscontainer\"\nblob_relative_path = \"ehe_national/\"\nblob_sas_token = r\"\"\n\n# Allow SPARK to read from Blob remotely\nwasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\nspark.conf.set(\n  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n  blob_sas_token)\nprint('Remote blob path: ' + wasbs_path)\n\n# SPARK read parquet, note that it won't load any data yet by now\ndf = spark.read.parquet(wasbs_path)\ndf.write.format(\"delta\").saveAsTable(\"us_employment_hours_earnings_national\"): Compute snapshot for version: 0","jobGroup":"3","jobId":7,"killedTasksSummary":{},"name":"$anonfun$recordDeltaOperation$5 at SynapseLoggingShim.scala:86","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":1,"rowCount":22,"stageIds":[10],"status":"SUCCEEDED","submissionTime":"2022-10-25T08:36:29.687GMT"},{"completionTime":"2022-10-25T08:36:28.588GMT","dataRead":70885090,"dataWritten":22376463,"description":"Job group for statement 3:\n# Azure storage access info\nblob_account_name = \"azureopendatastorage\"\nblob_container_name = \"laborstatisticscontainer\"\nblob_relative_path = \"ehe_national/\"\nblob_sas_token = r\"\"\n\n# Allow SPARK to read from Blob remotely\nwasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\nspark.conf.set(\n  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n  blob_sas_token)\nprint('Remote blob path: ' + wasbs_path)\n\n# SPARK read parquet, note that it won't load any data yet by now\ndf = spark.read.parquet(wasbs_path)\ndf.write.format(\"delta\").saveAsTable(\"us_employment_hours_earnings_national\")","jobGroup":"3","jobId":6,"killedTasksSummary":{},"name":"$anonfun$recordDeltaOperation$5 at SynapseLoggingShim.scala:86","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":9,"numCompletedStages":1,"numCompletedTasks":9,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":9,"rowCount":15284820,"stageIds":[9],"status":"SUCCEEDED","submissionTime":"2022-10-25T08:36:12.534GMT"},{"completionTime":"2022-10-25T08:36:09.693GMT","dataRead":0,"dataWritten":0,"description":"Job group for statement 3:\n# Azure storage access info\nblob_account_name = \"azureopendatastorage\"\nblob_container_name = \"laborstatisticscontainer\"\nblob_relative_path = \"ehe_national/\"\nblob_sas_token = r\"\"\n\n# Allow SPARK to read from Blob remotely\nwasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\nspark.conf.set(\n  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n  blob_sas_token)\nprint('Remote blob path: ' + wasbs_path)\n\n# SPARK read parquet, note that it won't load any data yet by now\ndf = spark.read.parquet(wasbs_path)\ndf.write.format(\"delta\").saveAsTable(\"us_employment_hours_earnings_national\")","jobGroup":"3","jobId":5,"killedTasksSummary":{},"name":"parquet at NativeMethodAccessorImpl.java:0","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":1,"rowCount":0,"stageIds":[8],"status":"SUCCEEDED","submissionTime":"2022-10-25T08:36:08.767GMT"}],"limit":20,"numbers":{"FAILED":0,"RUNNING":0,"SUCCEEDED":5,"UNKNOWN":0},"rule":"ALL_DESC"},"spark_pool":null,"state":"finished","statement_id":3},"text/plain":["StatementMeta(, d7adf307-1e22-40aa-8549-f8efa4c9564e, 3, Finished, Available)"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Remote blob path: wasbs://laborstatisticscontainer@azureopendatastorage.blob.core.windows.net/ehe_national/\n"]}],"source":["# Azure storage access info\n","blob_account_name = \"azureopendatastorage\"\n","blob_container_name = \"laborstatisticscontainer\"\n","blob_relative_path = \"ehe_national/\"\n","blob_sas_token = r\"\"\n","\n","# Allow SPARK to read from Blob remotely\n","wasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\n","spark.conf.set(\n","  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n","  blob_sas_token)\n","print('Remote blob path: ' + wasbs_path)\n","\n","# SPARK read parquet, note that it won't load any data yet by now\n","df = spark.read.parquet(wasbs_path)\n","df.write.format(\"delta\").saveAsTable(\"us_employment_hours_earnings_national\")"]},{"cell_type":"markdown","metadata":{"azdata_cell_guid":"5b199b98-3370-46dd-bc21-0b3c7c9e5183","nteract":{"transient":{"deleting":false}}},"source":["**US State Employment Hours and Earnings**\n","\n","The Current Employment Statistics (CES) program produces detailed industry estimates of nonfarm employment, hours, and earnings of workers on payrolls in the United States."]},{"cell_type":"code","execution_count":3,"metadata":{"azdata_cell_guid":"3d934f4c-287a-470e-a84b-baa9cb888433","jupyter":{"outputs_hidden":false,"source_hidden":false},"language":"python","nteract":{"transient":{"deleting":false}}},"outputs":[{"data":{"application/vnd.livy.statement-meta+json":{"execution_finish_time":"2022-10-25T08:37:08.2952549Z","execution_start_time":"2022-10-25T08:36:36.9986269Z","livy_statement_state":"available","queued_time":"2022-10-25T08:34:19.6708665Z","session_id":"d7adf307-1e22-40aa-8549-f8efa4c9564e","session_start_time":null,"spark_jobs":{"jobs":[{"completionTime":"2022-10-25T08:37:04.178GMT","dataRead":4538,"dataWritten":0,"description":"Delta: Job group for statement 4:\n# Azure storage access info\nblob_account_name = \"azureopendatastorage\"\nblob_container_name = \"laborstatisticscontainer\"\nblob_relative_path = \"ehe_state/\"\nblob_sas_token = r\"\"\n\n# Allow SPARK to read from Blob remotely\nwasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\nspark.conf.set(\n  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n  blob_sas_token)\nprint('Remote blob path: ' + wasbs_path)\n\n# SPARK read parquet, note that it won't load any data yet by now\ndf = spark.read.parquet(wasbs_path)\ndf.write.format(\"delta\").saveAsTable(\"us_employment_hours_earnings_state\"): Compute snapshot for version: 0","jobGroup":"4","jobId":14,"killedTasksSummary":{},"name":"$anonfun$recordDeltaOperation$5 at SynapseLoggingShim.scala:86","numActiveStages":-2,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":2,"numSkippedTasks":51,"numTasks":52,"rowCount":50,"stageIds":[21,22,23],"status":"SUCCEEDED","submissionTime":"2022-10-25T08:37:04.030GMT"},{"completionTime":"2022-10-25T08:37:03.990GMT","dataRead":6629,"dataWritten":4538,"description":"Delta: Job group for statement 4:\n# Azure storage access info\nblob_account_name = \"azureopendatastorage\"\nblob_container_name = \"laborstatisticscontainer\"\nblob_relative_path = \"ehe_state/\"\nblob_sas_token = r\"\"\n\n# Allow SPARK to read from Blob remotely\nwasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\nspark.conf.set(\n  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n  blob_sas_token)\nprint('Remote blob path: ' + wasbs_path)\n\n# SPARK read parquet, note that it won't load any data yet by now\ndf = spark.read.parquet(wasbs_path)\ndf.write.format(\"delta\").saveAsTable(\"us_employment_hours_earnings_state\"): Compute snapshot for version: 0","jobGroup":"4","jobId":13,"killedTasksSummary":{},"name":"$anonfun$recordDeltaOperation$5 at SynapseLoggingShim.scala:86","numActiveStages":-1,"numActiveTasks":0,"numCompletedIndices":50,"numCompletedStages":1,"numCompletedTasks":50,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":1,"numSkippedTasks":1,"numTasks":51,"rowCount":59,"stageIds":[19,20],"status":"SUCCEEDED","submissionTime":"2022-10-25T08:37:03.344GMT"},{"completionTime":"2022-10-25T08:37:03.120GMT","dataRead":10729,"dataWritten":6629,"description":"Delta: Job group for statement 4:\n# Azure storage access info\nblob_account_name = \"azureopendatastorage\"\nblob_container_name = \"laborstatisticscontainer\"\nblob_relative_path = \"ehe_state/\"\nblob_sas_token = r\"\"\n\n# Allow SPARK to read from Blob remotely\nwasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\nspark.conf.set(\n  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n  blob_sas_token)\nprint('Remote blob path: ' + wasbs_path)\n\n# SPARK read parquet, note that it won't load any data yet by now\ndf = spark.read.parquet(wasbs_path)\ndf.write.format(\"delta\").saveAsTable(\"us_employment_hours_earnings_state\"): Compute snapshot for version: 0","jobGroup":"4","jobId":12,"killedTasksSummary":{},"name":"$anonfun$recordDeltaOperation$5 at SynapseLoggingShim.scala:86","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":1,"rowCount":18,"stageIds":[18],"status":"SUCCEEDED","submissionTime":"2022-10-25T08:37:02.459GMT"},{"completionTime":"2022-10-25T08:37:00.034GMT","dataRead":47637897,"dataWritten":17249978,"description":"Job group for statement 4:\n# Azure storage access info\nblob_account_name = \"azureopendatastorage\"\nblob_container_name = \"laborstatisticscontainer\"\nblob_relative_path = \"ehe_state/\"\nblob_sas_token = r\"\"\n\n# Allow SPARK to read from Blob remotely\nwasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\nspark.conf.set(\n  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n  blob_sas_token)\nprint('Remote blob path: ' + wasbs_path)\n\n# SPARK read parquet, note that it won't load any data yet by now\ndf = spark.read.parquet(wasbs_path)\ndf.write.format(\"delta\").saveAsTable(\"us_employment_hours_earnings_state\")","jobGroup":"4","jobId":11,"killedTasksSummary":{},"name":"$anonfun$recordDeltaOperation$5 at SynapseLoggingShim.scala:86","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":7,"numCompletedStages":1,"numCompletedTasks":7,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":7,"rowCount":16178798,"stageIds":[17],"status":"SUCCEEDED","submissionTime":"2022-10-25T08:36:41.332GMT"},{"completionTime":"2022-10-25T08:36:38.345GMT","dataRead":0,"dataWritten":0,"description":"Job group for statement 4:\n# Azure storage access info\nblob_account_name = \"azureopendatastorage\"\nblob_container_name = \"laborstatisticscontainer\"\nblob_relative_path = \"ehe_state/\"\nblob_sas_token = r\"\"\n\n# Allow SPARK to read from Blob remotely\nwasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\nspark.conf.set(\n  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n  blob_sas_token)\nprint('Remote blob path: ' + wasbs_path)\n\n# SPARK read parquet, note that it won't load any data yet by now\ndf = spark.read.parquet(wasbs_path)\ndf.write.format(\"delta\").saveAsTable(\"us_employment_hours_earnings_state\")","jobGroup":"4","jobId":10,"killedTasksSummary":{},"name":"parquet at NativeMethodAccessorImpl.java:0","numActiveStages":0,"numActiveTasks":0,"numCompletedIndices":1,"numCompletedStages":1,"numCompletedTasks":1,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":1,"rowCount":0,"stageIds":[16],"status":"SUCCEEDED","submissionTime":"2022-10-25T08:36:37.613GMT"}],"limit":20,"numbers":{"FAILED":0,"RUNNING":0,"SUCCEEDED":5,"UNKNOWN":0},"rule":"ALL_DESC"},"spark_pool":null,"state":"finished","statement_id":4},"text/plain":["StatementMeta(, d7adf307-1e22-40aa-8549-f8efa4c9564e, 4, Finished, Available)"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Remote blob path: wasbs://laborstatisticscontainer@azureopendatastorage.blob.core.windows.net/ehe_state/\n"]}],"source":["# Azure storage access info\n","blob_account_name = \"azureopendatastorage\"\n","blob_container_name = \"laborstatisticscontainer\"\n","blob_relative_path = \"ehe_state/\"\n","blob_sas_token = r\"\"\n","\n","# Allow SPARK to read from Blob remotely\n","wasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\n","spark.conf.set(\n","  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n","  blob_sas_token)\n","print('Remote blob path: ' + wasbs_path)\n","\n","# SPARK read parquet, note that it won't load any data yet by now\n","df = spark.read.parquet(wasbs_path)\n","df.write.format(\"delta\").saveAsTable(\"us_employment_hours_earnings_state\")"]},{"cell_type":"markdown","metadata":{"azdata_cell_guid":"1c6b499e-3759-46ee-b4f2-84cfb1c4b1e4","nteract":{"transient":{"deleting":false}}},"source":["**US Local Area Unemployment Statistics Article**\n","\n","The Local Area Unemployment Statistics (LAUS) program produces monthly and annual employment, unemployment, and labor force data for Census regions and divisions, States, counties, metropolitan areas, and many cities in the United States."]},{"cell_type":"code","execution_count":4,"metadata":{"azdata_cell_guid":"9c96e3b0-cfe5-467c-a80d-7e92e3c167c2","jupyter":{"outputs_hidden":false,"source_hidden":false},"language":"python","nteract":{"transient":{"deleting":false}}},"outputs":[{"data":{"application/vnd.livy.statement-meta+json":{"execution_finish_time":null,"execution_start_time":"2022-10-25T08:37:08.3818444Z","livy_statement_state":"running","queued_time":"2022-10-25T08:34:19.6722727Z","session_id":"d7adf307-1e22-40aa-8549-f8efa4c9564e","session_start_time":null,"spark_jobs":{"jobs":[{"dataRead":0,"dataWritten":0,"description":"Job group for statement 5:\n# Azure storage access info\nblob_account_name = \"azureopendatastorage\"\nblob_container_name = \"laborstatisticscontainer\"\nblob_relative_path = \"laus/\"\nblob_sas_token = r\"\"\n\n# Allow SPARK to read from Blob remotely\nwasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\nspark.conf.set(\n  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n  blob_sas_token)\nprint('Remote blob path: ' + wasbs_path)\n\n# SPARK read parquet, note that it won't load any data yet by now\ndf = spark.read.parquet(wasbs_path)\ndf.write.format(\"delta\").saveAsTable(\"US Local Area Unemployment Statistics\")\n","jobGroup":"5","jobId":15,"killedTasksSummary":{},"name":"parquet at NativeMethodAccessorImpl.java:0","numActiveStages":1,"numActiveTasks":0,"numCompletedIndices":0,"numCompletedStages":0,"numCompletedTasks":0,"numFailedStages":0,"numFailedTasks":0,"numKilledTasks":0,"numSkippedStages":0,"numSkippedTasks":0,"numTasks":1,"rowCount":0,"stageIds":[24],"status":"RUNNING","submissionTime":"2022-10-25T08:37:08.998GMT"}],"limit":20,"numbers":{"FAILED":0,"RUNNING":1,"SUCCEEDED":0,"UNKNOWN":0},"rule":"ALL_DESC"},"spark_pool":null,"state":"submitted","statement_id":5},"text/plain":["StatementMeta(, d7adf307-1e22-40aa-8549-f8efa4c9564e, 5, Submitted, Running)"]},"metadata":{},"output_type":"display_data"}],"source":["# Azure storage access info\n","blob_account_name = \"azureopendatastorage\"\n","blob_container_name = \"laborstatisticscontainer\"\n","blob_relative_path = \"laus/\"\n","blob_sas_token = r\"\"\n","\n","# Allow SPARK to read from Blob remotely\n","wasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\n","spark.conf.set(\n","  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n","  blob_sas_token)\n","print('Remote blob path: ' + wasbs_path)\n","\n","# SPARK read parquet, note that it won't load any data yet by now\n","df = spark.read.parquet(wasbs_path)\n","df.write.format(\"delta\").saveAsTable(\"US Local Area Unemployment Statistics\")\n"]},{"cell_type":"markdown","metadata":{"azdata_cell_guid":"223f2879-f884-4834-8eb3-8f3970e78efb","nteract":{"transient":{"deleting":false}}},"source":["**US Consumer Price Index**\n","\n","The Consumer Price Index (CPI) is a measure of the average change over time in the prices paid by urban consumers for a market basket of consumer goods and services."]},{"cell_type":"code","execution_count":null,"metadata":{"azdata_cell_guid":"ce112714-f145-4295-9dce-cf1a4a105892","jupyter":{"outputs_hidden":false,"source_hidden":false},"language":"python","nteract":{"transient":{"deleting":false}}},"outputs":[{"data":{"application/vnd.livy.statement-meta+json":{"execution_finish_time":null,"execution_start_time":null,"livy_statement_state":null,"queued_time":"2022-10-25T08:34:19.6737691Z","session_id":null,"session_start_time":null,"spark_jobs":null,"spark_pool":null,"state":"waiting","statement_id":null},"text/plain":["StatementMeta(, , , Waiting, )"]},"metadata":{},"output_type":"display_data"}],"source":["# Azure storage access info\n","blob_account_name = \"azureopendatastorage\"\n","blob_container_name = \"laborstatisticscontainer\"\n","blob_relative_path = \"cpi/\"\n","blob_sas_token = r\"\"\n","\n","# Allow SPARK to read from Blob remotely\n","wasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\n","spark.conf.set(\n","  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n","  blob_sas_token)\n","print('Remote blob path: ' + wasbs_path)\n","\n","df = spark.read.parquet(wasbs_path)\n","df.write.format(\"delta\").saveAsTable(\"Us Consumer Price Index\")"]},{"cell_type":"markdown","metadata":{"azdata_cell_guid":"06cc940d-2600-4a55-8916-bb45775ef846","nteract":{"transient":{"deleting":false}}},"source":["**US Consumer Price Index - industry**\n","\n","The Producer Price Index (PPI) is a measure of average change over time in the selling prices received by domestic producers for their output. The prices included in the PPI are from the first commercial transaction for products and services covered."]},{"cell_type":"code","execution_count":null,"metadata":{"azdata_cell_guid":"9644bb6f-be92-4fe8-ad16-f75e75c36c6c","jupyter":{"outputs_hidden":false,"source_hidden":false},"language":"python","nteract":{"transient":{"deleting":false}}},"outputs":[{"data":{"application/vnd.livy.statement-meta+json":{"execution_finish_time":null,"execution_start_time":null,"livy_statement_state":null,"queued_time":"2022-10-25T08:34:19.6750699Z","session_id":null,"session_start_time":null,"spark_jobs":null,"spark_pool":null,"state":"waiting","statement_id":null},"text/plain":["StatementMeta(, , , Waiting, )"]},"metadata":{},"output_type":"display_data"}],"source":["# Azure storage access info\n","blob_account_name = \"azureopendatastorage\"\n","blob_container_name = \"laborstatisticscontainer\"\n","blob_relative_path = \"ppi_industry/\"\n","blob_sas_token = r\"\"\n","\n","# Allow SPARK to read from Blob remotely\n","wasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\n","spark.conf.set(\n","  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n","  blob_sas_token)\n","print('Remote blob path: ' + wasbs_path)\n","\n","df = spark.read.parquet(wasbs_path)\n","df.write.format(\"delta\").saveAsTable(\"Us Producer Price Index Industry\")"]},{"cell_type":"markdown","metadata":{"azdata_cell_guid":"4372f553-7730-48c6-843d-ce8087b3b627","nteract":{"transient":{"deleting":false}}},"source":["**US Producer Price Index - Commodities**\n","\n","The Producer Price Index (PPI) is a measure of average change over time in the selling prices received by domestic producers for their output. The prices included in the PPI are from the first commercial transaction for products and services covered."]},{"cell_type":"code","execution_count":null,"metadata":{"azdata_cell_guid":"095b1bd7-fbcc-42a2-aca6-28ac0b740934","jupyter":{"outputs_hidden":false,"source_hidden":false},"language":"python","nteract":{"transient":{"deleting":false}}},"outputs":[{"data":{"application/vnd.livy.statement-meta+json":{"execution_finish_time":null,"execution_start_time":null,"livy_statement_state":null,"queued_time":"2022-10-25T08:34:19.6763687Z","session_id":null,"session_start_time":null,"spark_jobs":null,"spark_pool":null,"state":"waiting","statement_id":null},"text/plain":["StatementMeta(, , , Waiting, )"]},"metadata":{},"output_type":"display_data"}],"source":["# Azure storage access info\n","blob_account_name = \"azureopendatastorage\"\n","blob_container_name = \"laborstatisticscontainer\"\n","blob_relative_path = \"ppi_commodity/\"\n","blob_sas_token = r\"\"\n","\n","# Allow SPARK to read from Blob remotely\n","wasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\n","spark.conf.set(\n","  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n","  blob_sas_token)\n","print('Remote blob path: ' + wasbs_path)\n","\n","df = spark.read.parquet(wasbs_path)\n","df.write.format(\"delta\").saveAsTable(\"Us Producer Price Index Commodities\")"]},{"cell_type":"markdown","metadata":{"azdata_cell_guid":"9aaabcab-7e09-411e-9bea-ca430cc005dc","nteract":{"transient":{"deleting":false}}},"source":["# Common datasets"]},{"cell_type":"markdown","metadata":{"azdata_cell_guid":"6f46c3c9-51a5-4323-87b6-e9b87dec6662","nteract":{"transient":{"deleting":false}}},"source":["## Public Holidays\n","\n","Worldwide public holiday data sourced from PyPI holidays package and Wikipedia, covering 38 countries or regions from 1970 to 2099."]},{"cell_type":"code","execution_count":null,"metadata":{"azdata_cell_guid":"f833d2e4-25fe-49da-b283-764dca14ab44","jupyter":{"outputs_hidden":false,"source_hidden":false},"language":"python","nteract":{"transient":{"deleting":false}}},"outputs":[{"data":{"application/vnd.livy.statement-meta+json":{"execution_finish_time":null,"execution_start_time":null,"livy_statement_state":null,"queued_time":"2022-10-25T08:34:19.6775899Z","session_id":null,"session_start_time":null,"spark_jobs":null,"spark_pool":null,"state":"waiting","statement_id":null},"text/plain":["StatementMeta(, , , Waiting, )"]},"metadata":{},"output_type":"display_data"}],"source":["# Azure storage access info\n","blob_account_name = \"azureopendatastorage\"\n","blob_container_name = \"holidaydatacontainer\"\n","blob_relative_path = \"Processed\"\n","blob_sas_token = r\"\"\n","\n","# Allow SPARK to read from Blob remotely\n","wasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\n","spark.conf.set(\n","  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n","  blob_sas_token)\n","print('Remote blob path: ' + wasbs_path)\n","\n","df = spark.read.parquet(wasbs_path)\n","df.write.format(\"delta\").saveAsTable(\"public_holidays\")"]},{"cell_type":"markdown","metadata":{},"source":["## US Population by County\n","\n","US population by gender and race for each US county sourced from 2000 and 2010 Decennial Census.\n","\n","This dataset is sourced from United States Census Bureau’s Decennial Census Dataset APIs. Review Terms of Service and Policies and Notices for the terms and conditions related to the use this dataset."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Azure storage access info\n","blob_account_name = \"azureopendatastorage\"\n","blob_container_name = \"censusdatacontainer\"\n","blob_relative_path = \"release/us_population_county/\"\n","blob_sas_token = r\"\"\n","\n","# Allow SPARK to read from Blob remotely\n","wasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\n","spark.conf.set(\n","  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n","  blob_sas_token)\n","print('Remote blob path: ' + wasbs_path)\n","\n","df = spark.read.parquet(wasbs_path)\n","df.write.format(\"delta\").saveAsTable(\"us_population_county\")"]},{"cell_type":"markdown","metadata":{},"source":["## US Population by ZIP code\n","\n","US population by gender and race for each US ZIP code sourced from 2000 and 2010 Decennial Census.\n","\n","This dataset is sourced from United States Census Bureau’s Decennial Census Dataset APIs. Review Terms of Service and Policies and Notices for the terms and conditions related to the use this dataset."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Azure storage access info\n","blob_account_name = \"azureopendatastorage\"\n","blob_container_name = \"censusdatacontainer\"\n","blob_relative_path = \"release/us_population_zip/\"\n","blob_sas_token = r\"\"\n","\n","# Allow SPARK to read from Blob remotely\n","wasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\n","spark.conf.set(\n","  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n","  blob_sas_token)\n","print('Remote blob path: ' + wasbs_path)\n","\n","df = spark.read.parquet(wasbs_path)\n","df.write.format(\"delta\").saveAsTable(\"us_population_zip\")\n"]}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"display_name":"Python 3.10.0 64-bit","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.0"},"notebook_environment":{},"save_output":true,"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.livy.synapse.ipythonInterpreter.enabled":"true"},"enableDebugMode":false,"keepAliveTimeout":30}},"synapse_widget":{"state":{},"version":"0.1"},"trident":{"lakehouse":{"default_lakehouse":"f39f8282-3e30-42f8-8085-fa466a51a845","default_lakehouse_name":"EconomicsLakehouse","known_lakehouses":[{"id":"f39f8282-3e30-42f8-8085-fa466a51a845"}]}},"vscode":{"interpreter":{"hash":"ad3ff54c1384f41d27445ebeca1522f5ac631ad8cbbc7eb0df89bf1abd899887"}}},"nbformat":4,"nbformat_minor":0}
