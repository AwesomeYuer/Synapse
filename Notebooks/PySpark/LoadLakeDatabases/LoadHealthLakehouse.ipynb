{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "bd92c41f-488a-47b3-b0f2-0028ff912ff7",
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "# Health data"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "28a7a6e5-c044-4cf3-8331-f6061bedebef",
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "## Diabetes dataset\n",
                "\n",
                "The Diabetes dataset has 442 samples with 10 features, making it ideal for getting started with machine learning algorithms. It's one of the most popular Scikit Learn Toy Datasets."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "azdata_cell_guid": "8e72ac44-04b9-4752-8639-f2b50433c3fe",
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "language": "python",
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "outputs": [],
            "source": [
                "# Azure storage access info\n",
                "blob_account_name = \"azureopendatastorage\"\n",
                "blob_container_name = \"mlsamples\"\n",
                "blob_relative_path = \"diabetes\"\n",
                "blob_sas_token = r\"\"\n",
                "\n",
                "# Allow SPARK to read from Blob remotely\n",
                "wasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\n",
                "spark.conf.set(\n",
                "  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n",
                "  blob_sas_token)\n",
                "print('Remote blob path: ' + wasbs_path)\n",
                "\n",
                "# SPARK read parquet, note that it won't load any data yet by now\n",
                "\n",
                "df = spark.read.parquet(wasbs_path)\n",
                "df.write.format(\"delta\").saveAsTable(\"diabetes\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "66d991e1-87ec-4eb2-8b4f-b39e692d6e8d",
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "## Bing COVID-19\n",
                "\n",
                "Bing COVID-19 data includes confirmed, fatal, and recovered cases from all regions, updated daily. This data is reflected in the Bing COVID-19 Tracker.\n",
                "\n",
                "Bing collects data from multiple trusted, reliable sources, including the World Health Organization (WHO), Centers for Disease Control and Prevention (CDC), national and state public health departments, BNO News, 24/7 Wall St., and Wikipedia."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "advisor": {
                    "adviceMetadata": "{\"artifactId\":\"34e516fb-9e8a-40ac-9778-42165e9aac3f\",\"activityId\":\"a32ae349-2404-45a3-bc20-59ce289cb0f7\",\"applicationId\":\"application_1665739271442_0001\",\"jobGroupId\":\"6\",\"advices\":{\"warn\":1}}"
                },
                "azdata_cell_guid": "4da243db-3730-4434-894a-a496efd7daa7",
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "language": "python",
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "outputs": [],
            "source": [
                "# Azure storage access info\n",
                "blob_account_name = \"pandemicdatalake\"\n",
                "blob_container_name = \"public\"\n",
                "blob_relative_path = \"curated/covid-19/bing_covid-19_data/latest/bing_covid-19_data.parquet\"\n",
                "blob_sas_token = r\"\"\n",
                "\n",
                "# Allow SPARK to read from Blob remotely\n",
                "wasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\n",
                "spark.conf.set(\n",
                "  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n",
                "  blob_sas_token)\n",
                "print('Remote blob path: ' + wasbs_path)\n",
                "\n",
                "df = spark.read.parquet(wasbs_path)\n",
                "df.write.format(\"delta\").saveAsTable(\"bing_covid\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "b7b5aaee-0ab7-43b4-8743-f2b1250db32a",
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "## European Centre for Disease Prevention and Control (ECDC) COVID-19 Cases\n",
                "\n",
                "The latest available public data on geographic distribution of COVID-19 cases worldwide from the European Center for Disease Prevention and Control (ECDC). Each row/entry contains the number of new cases reported per day and per country or region."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "azdata_cell_guid": "5557f67e-518d-4720-98e2-7ba5ff50b161",
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "language": "python",
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "outputs": [],
            "source": [
                "# Azure storage access info\n",
                "blob_account_name = \"pandemicdatalake\"\n",
                "blob_container_name = \"public\"\n",
                "blob_relative_path = \"curated/covid-19/ecdc_cases/latest/ecdc_cases.parquet\"\n",
                "blob_sas_token = r\"\"\n",
                "\n",
                "# Allow SPARK to read from Blob remotely\n",
                "wasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\n",
                "spark.conf.set(\n",
                "  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n",
                "  blob_sas_token)\n",
                "print('Remote blob path: ' + wasbs_path)\n",
                "\n",
                "# SPARK read parquet, note that it won't load any data yet by now\n",
                "df = spark.read.parquet(wasbs_path)\n",
                "df.write.format(\"delta\").saveAsTable(\"ecdc_covid\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "7c038bb7-5a59-42c6-aeb8-c04aab873507",
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "## COVID Tracking project\n",
                "\n",
                "The COVID Tracking Project dataset provides the latest numbers on tests, confirmed cases, hospitalizations, and patient outcomes from every US state and territory."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "azdata_cell_guid": "9ab63895-cf3a-469c-927b-95e3148a7264",
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "language": "python",
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "outputs": [],
            "source": [
                "# Azure storage access info\n",
                "blob_account_name = \"pandemicdatalake\"\n",
                "blob_container_name = \"public\"\n",
                "blob_relative_path = \"curated/covid-19/covid_tracking/latest/covid_tracking.parquet\"\n",
                "blob_sas_token = r\"\"\n",
                "\n",
                "# Allow SPARK to read from Blob remotely\n",
                "wasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\n",
                "spark.conf.set(\n",
                "  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n",
                "  blob_sas_token)\n",
                "print('Remote blob path: ' + wasbs_path)\n",
                "\n",
                "# SPARK read parquet, note that it won't load any data yet by now\n",
                "df = spark.read.parquet(wasbs_path)\n",
                "df.write.format(\"delta\").saveAsTable(\"covid_tracking\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "3bd26f19-1777-4482-9707-29a503d97f6f",
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "## Oxford COVID-19 Government Response Tracker\n",
                "\n",
                "The Oxford Covid-19 Government Response Tracker (OxCGRT) dataset contains systematic information on which governments have taken which measures, and when."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "azdata_cell_guid": "215a1bb7-34c8-4186-afc8-03d404980851",
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "language": "python",
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "outputs": [],
            "source": [
                "# Azure storage access info\n",
                "blob_account_name = \"pandemicdatalake\"\n",
                "blob_container_name = \"public\"\n",
                "blob_relative_path = \"curated/covid-19/covid_policy_tracker/latest/covid_policy_tracker.parquet\"\n",
                "blob_sas_token = r\"\"\n",
                "\n",
                "# Allow SPARK to read from Blob remotely\n",
                "wasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\n",
                "spark.conf.set(\n",
                "  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n",
                "  blob_sas_token)\n",
                "print('Remote blob path: ' + wasbs_path)\n",
                "\n",
                "df = spark.read.parquet(wasbs_path)\n",
                "df.write.format(\"delta\").saveAsTable(\"oxford_covid_tracking\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "9aaabcab-7e09-411e-9bea-ca430cc005dc",
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "# Common datasets"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "azdata_cell_guid": "6f46c3c9-51a5-4323-87b6-e9b87dec6662",
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "source": [
                "## Public Holidays\n",
                "\n",
                "Worldwide public holiday data sourced from PyPI holidays package and Wikipedia, covering 38 countries or regions from 1970 to 2099."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {
                "azdata_cell_guid": "f833d2e4-25fe-49da-b283-764dca14ab44",
                "jupyter": {
                    "outputs_hidden": false,
                    "source_hidden": false
                },
                "language": "python",
                "nteract": {
                    "transient": {
                        "deleting": false
                    }
                }
            },
            "outputs": [
                {
                    "data": {
                        "application/vnd.livy.statement-meta+json": {
                            "execution_finish_time": "2022-10-14T09:50:41.011985Z",
                            "execution_start_time": "2022-10-14T09:50:35.6677238Z",
                            "livy_statement_state": "available",
                            "queued_time": "2022-10-14T09:39:09.8309855Z",
                            "session_id": "a32ae349-2404-45a3-bc20-59ce289cb0f7",
                            "session_start_time": null,
                            "spark_jobs": {
                                "jobs": [
                                    {
                                        "completionTime": "2022-10-14T09:50:40.289GMT",
                                        "dataRead": 4404,
                                        "dataWritten": 0,
                                        "description": "Delta: Job group for statement 17:\n# Azure storage access info\nblob_account_name = \"azureopendatastorage\"\nblob_container_name = \"holidaydatacontainer\"\nblob_relative_path = \"Processed\"\nblob_sas_token = r\"\"\n\n# Allow SPARK to read from Blob remotely\nwasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\nspark.conf.set(\n  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n  blob_sas_token)\nprint('Remote blob path: ' + wasbs_path)\n\ndf = spark.read.parquet(wasbs_path)\ndf.write.format(\"delta\").save(\"Tables/public_holidays\"): Compute snapshot for version: 0",
                                        "jobGroup": "17",
                                        "jobId": 80,
                                        "killedTasksSummary": {},
                                        "name": "$anonfun$recordDeltaOperation$5 at SynapseLoggingShim.scala:86",
                                        "numActiveStages": -2,
                                        "numActiveTasks": 0,
                                        "numCompletedIndices": 1,
                                        "numCompletedStages": 1,
                                        "numCompletedTasks": 1,
                                        "numFailedStages": 0,
                                        "numFailedTasks": 0,
                                        "numKilledTasks": 0,
                                        "numSkippedStages": 2,
                                        "numSkippedTasks": 51,
                                        "numTasks": 52,
                                        "rowCount": 50,
                                        "stageIds": [
                                            126,
                                            127,
                                            128
                                        ],
                                        "status": "SUCCEEDED",
                                        "submissionTime": "2022-10-14T09:50:40.153GMT"
                                    },
                                    {
                                        "completionTime": "2022-10-14T09:50:40.126GMT",
                                        "dataRead": 1631,
                                        "dataWritten": 4404,
                                        "description": "Delta: Job group for statement 17:\n# Azure storage access info\nblob_account_name = \"azureopendatastorage\"\nblob_container_name = \"holidaydatacontainer\"\nblob_relative_path = \"Processed\"\nblob_sas_token = r\"\"\n\n# Allow SPARK to read from Blob remotely\nwasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\nspark.conf.set(\n  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n  blob_sas_token)\nprint('Remote blob path: ' + wasbs_path)\n\ndf = spark.read.parquet(wasbs_path)\ndf.write.format(\"delta\").save(\"Tables/public_holidays\"): Compute snapshot for version: 0",
                                        "jobGroup": "17",
                                        "jobId": 79,
                                        "killedTasksSummary": {},
                                        "name": "$anonfun$recordDeltaOperation$5 at SynapseLoggingShim.scala:86",
                                        "numActiveStages": -1,
                                        "numActiveTasks": 0,
                                        "numCompletedIndices": 50,
                                        "numCompletedStages": 1,
                                        "numCompletedTasks": 50,
                                        "numFailedStages": 0,
                                        "numFailedTasks": 0,
                                        "numKilledTasks": 0,
                                        "numSkippedStages": 1,
                                        "numSkippedTasks": 1,
                                        "numTasks": 51,
                                        "rowCount": 54,
                                        "stageIds": [
                                            125,
                                            124
                                        ],
                                        "status": "SUCCEEDED",
                                        "submissionTime": "2022-10-14T09:50:39.964GMT"
                                    },
                                    {
                                        "completionTime": "2022-10-14T09:50:39.819GMT",
                                        "dataRead": 1916,
                                        "dataWritten": 1631,
                                        "description": "Delta: Job group for statement 17:\n# Azure storage access info\nblob_account_name = \"azureopendatastorage\"\nblob_container_name = \"holidaydatacontainer\"\nblob_relative_path = \"Processed\"\nblob_sas_token = r\"\"\n\n# Allow SPARK to read from Blob remotely\nwasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\nspark.conf.set(\n  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n  blob_sas_token)\nprint('Remote blob path: ' + wasbs_path)\n\ndf = spark.read.parquet(wasbs_path)\ndf.write.format(\"delta\").save(\"Tables/public_holidays\"): Compute snapshot for version: 0",
                                        "jobGroup": "17",
                                        "jobId": 78,
                                        "killedTasksSummary": {},
                                        "name": "$anonfun$recordDeltaOperation$5 at SynapseLoggingShim.scala:86",
                                        "numActiveStages": 0,
                                        "numActiveTasks": 0,
                                        "numCompletedIndices": 1,
                                        "numCompletedStages": 1,
                                        "numCompletedTasks": 1,
                                        "numFailedStages": 0,
                                        "numFailedTasks": 0,
                                        "numKilledTasks": 0,
                                        "numSkippedStages": 0,
                                        "numSkippedTasks": 0,
                                        "numTasks": 1,
                                        "rowCount": 8,
                                        "stageIds": [
                                            123
                                        ],
                                        "status": "SUCCEEDED",
                                        "submissionTime": "2022-10-14T09:50:39.508GMT"
                                    },
                                    {
                                        "completionTime": "2022-10-14T09:50:38.693GMT",
                                        "dataRead": 331687,
                                        "dataWritten": 357946,
                                        "description": "Job group for statement 17:\n# Azure storage access info\nblob_account_name = \"azureopendatastorage\"\nblob_container_name = \"holidaydatacontainer\"\nblob_relative_path = \"Processed\"\nblob_sas_token = r\"\"\n\n# Allow SPARK to read from Blob remotely\nwasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\nspark.conf.set(\n  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n  blob_sas_token)\nprint('Remote blob path: ' + wasbs_path)\n\ndf = spark.read.parquet(wasbs_path)\ndf.write.format(\"delta\").save(\"Tables/public_holidays\")",
                                        "jobGroup": "17",
                                        "jobId": 77,
                                        "killedTasksSummary": {},
                                        "name": "save at NativeMethodAccessorImpl.java:0",
                                        "numActiveStages": 0,
                                        "numActiveTasks": 0,
                                        "numCompletedIndices": 1,
                                        "numCompletedStages": 1,
                                        "numCompletedTasks": 1,
                                        "numFailedStages": 0,
                                        "numFailedTasks": 0,
                                        "numKilledTasks": 0,
                                        "numSkippedStages": 0,
                                        "numSkippedTasks": 0,
                                        "numTasks": 1,
                                        "rowCount": 139114,
                                        "stageIds": [
                                            122
                                        ],
                                        "status": "SUCCEEDED",
                                        "submissionTime": "2022-10-14T09:50:37.060GMT"
                                    },
                                    {
                                        "completionTime": "2022-10-14T09:50:36.666GMT",
                                        "dataRead": 0,
                                        "dataWritten": 0,
                                        "description": "Job group for statement 17:\n# Azure storage access info\nblob_account_name = \"azureopendatastorage\"\nblob_container_name = \"holidaydatacontainer\"\nblob_relative_path = \"Processed\"\nblob_sas_token = r\"\"\n\n# Allow SPARK to read from Blob remotely\nwasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\nspark.conf.set(\n  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n  blob_sas_token)\nprint('Remote blob path: ' + wasbs_path)\n\ndf = spark.read.parquet(wasbs_path)\ndf.write.format(\"delta\").save(\"Tables/public_holidays\")",
                                        "jobGroup": "17",
                                        "jobId": 76,
                                        "killedTasksSummary": {},
                                        "name": "parquet at NativeMethodAccessorImpl.java:0",
                                        "numActiveStages": 0,
                                        "numActiveTasks": 0,
                                        "numCompletedIndices": 1,
                                        "numCompletedStages": 1,
                                        "numCompletedTasks": 1,
                                        "numFailedStages": 0,
                                        "numFailedTasks": 0,
                                        "numKilledTasks": 0,
                                        "numSkippedStages": 0,
                                        "numSkippedTasks": 0,
                                        "numTasks": 1,
                                        "rowCount": 0,
                                        "stageIds": [
                                            121
                                        ],
                                        "status": "SUCCEEDED",
                                        "submissionTime": "2022-10-14T09:50:36.339GMT"
                                    }
                                ],
                                "limit": 20,
                                "numbers": {
                                    "FAILED": 0,
                                    "RUNNING": 0,
                                    "SUCCEEDED": 5,
                                    "UNKNOWN": 0
                                },
                                "rule": "ALL_DESC"
                            },
                            "spark_pool": null,
                            "state": "finished",
                            "statement_id": 17
                        },
                        "text/plain": [
                            "StatementMeta(, a32ae349-2404-45a3-bc20-59ce289cb0f7, 17, Finished, Available)"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Remote blob path: wasbs://holidaydatacontainer@azureopendatastorage.blob.core.windows.net/Processed\n"
                    ]
                }
            ],
            "source": [
                "# Azure storage access info\n",
                "blob_account_name = \"azureopendatastorage\"\n",
                "blob_container_name = \"holidaydatacontainer\"\n",
                "blob_relative_path = \"Processed\"\n",
                "blob_sas_token = r\"\"\n",
                "\n",
                "# Allow SPARK to read from Blob remotely\n",
                "wasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\n",
                "spark.conf.set(\n",
                "  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n",
                "  blob_sas_token)\n",
                "print('Remote blob path: ' + wasbs_path)\n",
                "\n",
                "df = spark.read.parquet(wasbs_path)\n",
                "df.write.format(\"delta\").saveAsTable(\"public_holidays\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## US Population by County\n",
                "\n",
                "US population by gender and race for each US county sourced from 2000 and 2010 Decennial Census.\n",
                "\n",
                "This dataset is sourced from United States Census Bureau’s Decennial Census Dataset APIs. Review Terms of Service and Policies and Notices for the terms and conditions related to the use this dataset."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Azure storage access info\n",
                "blob_account_name = \"azureopendatastorage\"\n",
                "blob_container_name = \"censusdatacontainer\"\n",
                "blob_relative_path = \"release/us_population_county/\"\n",
                "blob_sas_token = r\"\"\n",
                "\n",
                "# Allow SPARK to read from Blob remotely\n",
                "wasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\n",
                "spark.conf.set(\n",
                "  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n",
                "  blob_sas_token)\n",
                "print('Remote blob path: ' + wasbs_path)\n",
                "\n",
                "df = spark.read.parquet(wasbs_path)\n",
                "df.write.format(\"delta\").saveAsTable(\"us_population_county\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## US Population by ZIP code\n",
                "\n",
                "US population by gender and race for each US ZIP code sourced from 2000 and 2010 Decennial Census.\n",
                "\n",
                "This dataset is sourced from United States Census Bureau’s Decennial Census Dataset APIs. Review Terms of Service and Policies and Notices for the terms and conditions related to the use this dataset."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Azure storage access info\n",
                "blob_account_name = \"azureopendatastorage\"\n",
                "blob_container_name = \"censusdatacontainer\"\n",
                "blob_relative_path = \"release/us_population_zip/\"\n",
                "blob_sas_token = r\"\"\n",
                "\n",
                "# Allow SPARK to read from Blob remotely\n",
                "wasbs_path = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\n",
                "spark.conf.set(\n",
                "  'fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),\n",
                "  blob_sas_token)\n",
                "print('Remote blob path: ' + wasbs_path)\n",
                "\n",
                "df = spark.read.parquet(wasbs_path)\n",
                "df.write.format(\"delta\").saveAsTable(\"us_population_zip\")\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.10.0 64-bit",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        },
        "notebook_environment": {},
        "save_output": true,
        "spark_compute": {
            "compute_id": "/trident/default",
            "session_options": {
                "conf": {
                    "spark.livy.synapse.ipythonInterpreter.enabled": "true"
                },
                "enableDebugMode": false,
                "keepAliveTimeout": 30
            }
        },
        "synapse_widget": {
            "state": {},
            "version": "0.1"
        },
        "trident": {
            "lakehouse": {
                "default_lakehouse": "b6640c3e-07ce-482d-8dda-4a909a6be05c",
                "default_lakehouse_name": "HealthLakehouse",
                "known_lakehouses": [
                    {
                        "id": "b6640c3e-07ce-482d-8dda-4a909a6be05c"
                    }
                ]
            }
        },
        "vscode": {
            "interpreter": {
                "hash": "ad3ff54c1384f41d27445ebeca1522f5ac631ad8cbbc7eb0df89bf1abd899887"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
