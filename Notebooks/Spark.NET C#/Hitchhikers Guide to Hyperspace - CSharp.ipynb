{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Hitchhiker's Guide to Hyperspace (.NET for Spark C#)\n",
        "## An Indexing Subsystem for Apache Spark™\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/rapoth/hyperspace/master/docs/assets/images/hyperspace-small-banner.png\" alt=\"Hyperspace Indexing Sub-System Logo\" width=\"1000\"/>\n",
        "\n",
        "[Hyperspace](https://github.com/microsoft/hyperspace) introduces the ability for Apache Spark™ users to create indexes on their datasets (e.g., CSV, JSON, Parquet etc.) and leverage them for potential query and workload acceleration.\n",
        "\n",
        "In this notebook, we highlight the basics of Hyperspace, emphasizing on its simplicity and show how it can be used by just anyone.\n",
        "\n",
        "**Disclaimer**: Hyperspace helps accelerate your workloads/queries under two circumstances:\n",
        "\n",
        "  1. Queries contain filters on predicates with high selectivity (e.g., you want to select 100 matching rows from a million candidate rows)\n",
        "  2. Queries contain a join that requires heavy-shuffles (e.g., you want to join a 100 GB dataset with a 10 GB dataset)\n",
        "\n",
        "You may want to carefully monitor your workloads and determine whether indexing is helping you on a case-by-case basis."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup\n",
        "To begin with, let's start a new Spark™ session. Since this notebook is a tutorial merely to illustrate what Hyperspace can offer, we will make a configuration change that allow us to highlight what Hyperspace is doing on small datasets. By default, Spark™ uses *broadcast join* to optimize join queries when the data size for one side of join is small (which is the case for the sample data we use in this tutorial). Therefore, we disable broadcast joins so that later when we run join queries, Spark™ uses *sort-merge* join. This is mainly to show how Hyperspace indexes would be used at scale for accelerating join queries.\n",
        "\n",
        "The output of running the cell below shows a reference to the successfully created Spark™ session and prints out '-1' as the value for the modified join config which indicates that broadcast join is successfully disabled."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "var sessionId = (new Random()).Next(10000000);\n",
        "var dataPath = $\"/hyperspace/data-{sessionId}\";\n",
        "var indexLocation = $\"/hyperspace/indexes-{sessionId}\";\n",
        "\n",
        "// Please note that you DO NOT need to change this configuration in production.\n",
        "// We store all indexes in the system folder within Synapse.\n",
        "spark.Conf().Set(\"spark.hyperspace.system.path\", indexLocation);"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "medium",
              "session_id": 21,
              "statement_id": 1,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-03-15T07:01:20.6374448Z",
              "execution_start_time": "2021-03-15T07:02:10.1639212Z",
              "execution_finish_time": "2021-03-15T07:02:12.2302351Z"
            },
            "text/plain": "StatementMeta(medium, 21, 1, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 1,
          "data": {
            "text/plain": ""
          },
          "metadata": {}
        }
      ],
      "execution_count": 1,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "// Disable BroadcastHashJoin, so Spark™ will use standard SortMergeJoin. Currently hyperspace indexes utilize SortMergeJoin to speed up query.\n",
        "spark.Conf().Set(\"spark.sql.autoBroadcastJoinThreshold\", -1);\n",
        "\n",
        "// Verify that BroadcastHashJoin is set correctly \n",
        "Console.WriteLine(spark.Conf().Get(\"spark.sql.autoBroadcastJoinThreshold\"));\n",
        "\n",
        "spark.Conf().Set(\"spark.hyperspace.explain.displayMode\", \"html\")"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "medium",
              "session_id": 21,
              "statement_id": 2,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-03-15T07:01:20.63933Z",
              "execution_start_time": "2021-03-15T07:02:12.318093Z",
              "execution_finish_time": "2021-03-15T07:02:14.3926254Z"
            },
            "text/plain": "StatementMeta(medium, 21, 2, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 2,
          "data": {
            "text/plain": "-1\n"
          },
          "metadata": {}
        }
      ],
      "execution_count": 2,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preparation\n",
        "\n",
        "To prepare our environment, we will create sample data records and save them as parquet data files. While we use Parquet for illustration, you can use other formats such as CSV. In the subsequent cells, we will also demonstrate how you can create several Hyperspace indexes on this sample dataset and how one can make Spark™ use them when running queries. \n",
        "\n",
        "Our example records correspond to two datasets: *department* and *employee*. You should configure \"empLocation\" and \"deptLocation\" paths so that on the storage account they point to your desired location to save generated data files. \n",
        "\n",
        "The output of running below cell shows contents of our datasets as lists of triplets followed by references to dataFrames created to save the content of each dataset in our preferred location."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "using Microsoft.Spark.Sql.Types;\n",
        "\n",
        "// Sample department records\n",
        "var departments = new List<GenericRow>()\n",
        "{\n",
        "    new GenericRow(new object[] {10, \"Accounting\", \"New York\"}),\n",
        "    new GenericRow(new object[] {20, \"Research\", \"Dallas\"}),\n",
        "    new GenericRow(new object[] {30, \"Sales\", \"Chicago\"}),\n",
        "    new GenericRow(new object[] {40, \"Operations\", \"Boston\"})\n",
        "};\n",
        "\n",
        "// Sample employee records\n",
        "var employees = new List<GenericRow>() {\n",
        "      new GenericRow(new object[] {7369, \"SMITH\", 20}),\n",
        "      new GenericRow(new object[] {7499, \"ALLEN\", 30}),\n",
        "      new GenericRow(new object[] {7521, \"WARD\", 30}),\n",
        "      new GenericRow(new object[] {7566, \"JONES\", 20}),\n",
        "      new GenericRow(new object[] {7698, \"BLAKE\", 30}),\n",
        "      new GenericRow(new object[] {7782, \"CLARK\", 10}),\n",
        "      new GenericRow(new object[] {7788, \"SCOTT\", 20}),\n",
        "      new GenericRow(new object[] {7839, \"KING\", 10}),\n",
        "      new GenericRow(new object[] {7844, \"TURNER\", 30}),\n",
        "      new GenericRow(new object[] {7876, \"ADAMS\", 20}),\n",
        "      new GenericRow(new object[] {7900, \"JAMES\", 30}),\n",
        "      new GenericRow(new object[] {7934, \"MILLER\", 10}),\n",
        "      new GenericRow(new object[] {7902, \"FORD\", 20}),\n",
        "      new GenericRow(new object[] {7654, \"MARTIN\", 30})\n",
        "};\n",
        "\n",
        "// Save sample data in the Parquet format\n",
        "var departmentSchema = new StructType(new List<StructField>()\n",
        "{\n",
        "    new StructField(\"deptId\", new IntegerType()),\n",
        "    new StructField(\"deptName\", new StringType()),\n",
        "    new StructField(\"location\", new StringType())\n",
        "});\n",
        "var employeeSchema = new StructType(new List<StructField>()\n",
        "{\n",
        "    new StructField(\"empId\", new IntegerType()),\n",
        "    new StructField(\"empName\", new StringType()),\n",
        "    new StructField(\"deptId\", new IntegerType())\n",
        "});\n",
        "\n",
        "DataFrame empData = spark.CreateDataFrame(employees, employeeSchema); \n",
        "DataFrame deptData = spark.CreateDataFrame(departments, departmentSchema); \n",
        "\n",
        "string empLocation = $\"{dataPath}/employees.parquet\";\n",
        "string deptLocation = $\"{dataPath}/departments.parquet\";\n",
        "empData.Write().Mode(\"overwrite\").Parquet(empLocation);\n",
        "deptData.Write().Mode(\"overwrite\").Parquet(deptLocation);"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "medium",
              "session_id": 21,
              "statement_id": 3,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-03-15T07:01:20.6410577Z",
              "execution_start_time": "2021-03-15T07:02:14.4857898Z",
              "execution_finish_time": "2021-03-15T07:02:26.8576441Z"
            },
            "text/plain": "StatementMeta(medium, 21, 3, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 3,
          "data": {
            "text/plain": ""
          },
          "metadata": {}
        }
      ],
      "execution_count": 3,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's verify the contents of parquet files we created above to make sure they contain expected records in correct format. We later use these data files to create Hyperspace indexes and run sample queries.\n",
        "\n",
        "Running below cell, the output displays the rows in employee and department dataframes in a tabular form. There should be 14 employees and 4 departments, each matching with one of triplets we created in the previous cell."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "// empLocation and deptLocation are the user defined locations above to save parquet files\n",
        "DataFrame empDF = spark.Read().Parquet(empLocation);\n",
        "DataFrame deptDF = spark.Read().Parquet(deptLocation);\n",
        "\n",
        "// Verify the data is available and correct\n",
        "empDF.Show();\n",
        "deptDF.Show();"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "medium",
              "session_id": 21,
              "statement_id": 4,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-03-15T07:01:20.6431547Z",
              "execution_start_time": "2021-03-15T07:02:26.9437915Z",
              "execution_finish_time": "2021-03-15T07:02:31.0706367Z"
            },
            "text/plain": "StatementMeta(medium, 21, 4, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 4,
          "data": {
            "text/plain": "+-----+-------+------+\n|empId|empName|deptId|\n+-----+-------+------+\n| 7844| TURNER|    30|\n| 7934| MILLER|    10|\n| 7654| MARTIN|    30|\n| 7369|  SMITH|    20|\n| 7499|  ALLEN|    30|\n| 7566|  JONES|    20|\n| 7698|  BLAKE|    30|\n| 7782|  CLARK|    10|\n| 7788|  SCOTT|    20|\n| 7876|  ADAMS|    20|\n| 7900|  JAMES|    30|\n| 7521|   WARD|    30|\n| 7839|   KING|    10|\n| 7902|   FORD|    20|\n+-----+-------+------+\n\n+------+----------+--------+\n|deptId|  deptName|location|\n+------+----------+--------+\n|    10|Accounting|New York|\n|    40|Operations|  Boston|\n|    20|  Research|  Dallas|\n|    30|     Sales| Chicago|\n+------+----------+--------+\n\n"
          },
          "metadata": {}
        }
      ],
      "execution_count": 4,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hello Hyperspace Index!\n",
        "Hyperspace lets users create indexes on records scanned from persisted data files. Once successfully created, an entry corresponding to the index is added to the Hyperspace's metadata. This metadata is later used by Apache Spark™'s Hyperspace-enabled optimizer during query processing to find and use proper indexes. \n",
        "\n",
        "Once indexes are created, users can perform several actions:\n",
        "  - **Refresh** If the underlying data changes, users can refresh an existing index to capture that. \n",
        "  - **Delete** If the index is not needed, users can perform a soft-delete i.e., index is not physically deleted but is marked as 'deleted' so it is no longer used in your workloads.\n",
        "  - **Vacuum** If an index is no longer required, users can vacuum it which forces a physical deletion of the index contents and associated metadata completely from Hyperspace's metadata.\n",
        "\n",
        "Below sections show how such index management operations can be done in Hyperspace.\n",
        "\n",
        "First, we need to import the required libraries and create an instance of Hyperspace. We later use this instance to invoke different Hyperspace APIs to create indexes on our sample data and modify those indexes.\n",
        "\n",
        "Output of running below cell shows a reference to the created instance of Hyperspace."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "// Create an instance of Hyperspace\n",
        "using Microsoft.Spark.Extensions.Hyperspace;\n",
        "\n",
        "Hyperspace hyperspace = new Hyperspace(spark);"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "medium",
              "session_id": 21,
              "statement_id": 5,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-03-15T07:01:20.6452708Z",
              "execution_start_time": "2021-03-15T07:02:31.1570981Z",
              "execution_finish_time": "2021-03-15T07:02:33.2176758Z"
            },
            "text/plain": "StatementMeta(medium, 21, 5, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 5,
          "data": {
            "text/plain": ""
          },
          "metadata": {}
        }
      ],
      "execution_count": 5,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Indexes\n",
        "To create a Hyperspace index, the user needs to provide 2 pieces of information:\n",
        "* An Apache Spark™ DataFrame which references the data to be indexed.\n",
        "* An index configuration object: IndexConfig, which specifies the *index name*, *indexed* and *included* columns of the index. \n",
        "\n",
        "As you might have noticed, in this notebook, we illustrate indexing using the [Covering Index](https://www.red-gate.com/simple-talk/sql/learn-sql-server/using-covering-indexes-to-improve-query-performance/), which are the default index in Hyperspace. In the future, we plan on adding support for other index types. \n",
        "\n",
        "We start by creating three Hyperspace indexes on our sample data: two indexes on the department dataset named \"deptIndex1\" and \"deptIndex2\", and one index on the employee dataset named 'empIndex'. \n",
        "For each index, we need a corresponding IndexConfig to capture the name along with columns lists for the indexed and included columns. Running below cell creates these indexConfigs and its output lists them.\n",
        "\n",
        "**Note**: An *index column* is a column that appears in your filters or join conditions. An *included column* is a column that appears in your select/project.\n",
        "\n",
        "For instance, in the following query:\n",
        "```sql\n",
        "SELECT X\n",
        "FROM Table\n",
        "WHERE Y = 2\n",
        "```\n",
        "X can be an *index column* and Y can be an *included column*."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "// Create index configurations\n",
        "using Microsoft.Spark.Extensions.Hyperspace.Index;\n",
        "\n",
        "var empIndexConfig = new IndexConfig(\"empIndex\", new string[] {\"deptId\"}, new string[] {\"empName\"});\n",
        "var deptIndexConfig1 = new IndexConfig(\"deptIndex1\", new string[] {\"deptId\"}, new string[] {\"deptName\"});\n",
        "var deptIndexConfig2 = new IndexConfig(\"deptIndex2\", new string[] {\"location\"}, new string[] {\"deptName\"});"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "medium",
              "session_id": 21,
              "statement_id": 6,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-03-15T07:01:20.6481726Z",
              "execution_start_time": "2021-03-15T07:02:33.3036687Z",
              "execution_finish_time": "2021-03-15T07:02:35.353965Z"
            },
            "text/plain": "StatementMeta(medium, 21, 6, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 6,
          "data": {
            "text/plain": ""
          },
          "metadata": {}
        }
      ],
      "execution_count": 6,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we create three indexes using our index configurations. For this purpose, we invoke \"createIndex\" command on our Hyperspace instance. This command requires an index configuration and the dataFrame containing rows to be indexed.\n",
        "Running below cell creates three indexes.\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "// Create indexes from configurations\n",
        "hyperspace.CreateIndex(empDF, empIndexConfig);\n",
        "hyperspace.CreateIndex(deptDF, deptIndexConfig1);\n",
        "hyperspace.CreateIndex(deptDF, deptIndexConfig2);"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "medium",
              "session_id": 21,
              "statement_id": 7,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-03-15T07:01:20.6504511Z",
              "execution_start_time": "2021-03-15T07:02:35.4361305Z",
              "execution_finish_time": "2021-03-15T07:02:43.6645462Z"
            },
            "text/plain": "StatementMeta(medium, 21, 7, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 7,
          "data": {
            "text/plain": ""
          },
          "metadata": {}
        }
      ],
      "execution_count": 7,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### List Indexes\n",
        "\n",
        "Below code shows how a user can list all available indexes in a Hyperspace instance. It uses the `indexes` API which returns information about existing indexes as a Spark™'s DataFrame so you can perform additional operations. For instance, you can invoke valid operations on this DataFrame for checking its content or analyzing it further (for example filtering specific indexes or grouping them according to some desired property). \n",
        "\n",
        "Below cell uses DataFrame's `show` action to fully print the rows and show details of our indexes in a tabular form. For each index, we can see all the information Hyperspace has stored about it in its metadata. \n",
        "\n",
        "You will immediately notice the following:\n",
        "  - `config.indexName`, `config.indexedColumns`, `config.includedColumns` are the fields that a user normally provides during index creation.\n",
        "  - `status.status` indicates if the index is being actively used by the Spark's optimizer.\n",
        "  - `dfSignature` is automatically generated by Hyperspace and is unique for each index. Hyperspace uses this signature internally to maintain the index and exploit it at query time. \n",
        "  \n",
        "In the output below, all three indexes should have \"ACTIVE\" as status and their name, indexed columns, and included columns should match with what we defined in index configurations above."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "hyperspace.Indexes().Show();"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "medium",
              "session_id": 21,
              "statement_id": 8,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-03-15T07:01:20.6524534Z",
              "execution_start_time": "2021-03-15T07:02:43.7480983Z",
              "execution_finish_time": "2021-03-15T07:02:45.8076969Z"
            },
            "text/plain": "StatementMeta(medium, 21, 8, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 8,
          "data": {
            "text/plain": "+----------+--------------+---------------+----------+--------------------+--------------------+------+\n|      name|indexedColumns|includedColumns|numBuckets|              schema|       indexLocation| state|\n+----------+--------------+---------------+----------+--------------------+--------------------+------+\n|deptIndex1|      [deptId]|     [deptName]|       200|{\"type\":\"struct\",...|abfss://default@h...|ACTIVE|\n|deptIndex2|    [location]|     [deptName]|       200|{\"type\":\"struct\",...|abfss://default@h...|ACTIVE|\n|  empIndex|      [deptId]|      [empName]|       200|{\"type\":\"struct\",...|abfss://default@h...|ACTIVE|\n+----------+--------------+---------------+----------+--------------------+--------------------+------+\n\n"
          },
          "metadata": {}
        }
      ],
      "execution_count": 8,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Delete Indexes\n",
        "A user can drop an existing index by using the `deleteIndex` API and providing the index name. \n",
        "\n",
        "Index deletion is a **soft-delete** operation i.e., only the index's status in the Hyperspace metadata from is changed from \"ACTIVE\" to \"DELETED\". This will exclude the deleted index from any future query optimization and Hyperspace no longer picks that index for any query. However, index files for a deleted index still remain available (since it is a soft-delete), so if you accidentally deleted the index, you could still restore it.\n",
        "\n",
        "The cell below deletes index with name \"deptIndex2\" and lists Hyperspace metadata after that. The output should be similar to above cell for \"List Indexes\" except for \"deptIndex2\" which now should have its status changed into \"DELETED\"."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "hyperspace.DeleteIndex(\"deptIndex2\");\n",
        "\n",
        "hyperspace.Indexes().Show();"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "medium",
              "session_id": 21,
              "statement_id": 9,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-03-15T07:01:20.6546613Z",
              "execution_start_time": "2021-03-15T07:02:45.8933365Z",
              "execution_finish_time": "2021-03-15T07:02:47.9445707Z"
            },
            "text/plain": "StatementMeta(medium, 21, 9, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 9,
          "data": {
            "text/plain": "+----------+--------------+---------------+----------+--------------------+--------------------+-------+\n|      name|indexedColumns|includedColumns|numBuckets|              schema|       indexLocation|  state|\n+----------+--------------+---------------+----------+--------------------+--------------------+-------+\n|deptIndex1|      [deptId]|     [deptName]|       200|{\"type\":\"struct\",...|abfss://default@h...| ACTIVE|\n|deptIndex2|    [location]|     [deptName]|       200|{\"type\":\"struct\",...|abfss://default@h...|DELETED|\n|  empIndex|      [deptId]|      [empName]|       200|{\"type\":\"struct\",...|abfss://default@h...| ACTIVE|\n+----------+--------------+---------------+----------+--------------------+--------------------+-------+\n\n"
          },
          "metadata": {}
        }
      ],
      "execution_count": 9,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Restore Indexes\n",
        "A user can use the `restoreIndex` API to restore a deleted index. This will bring back the latest version of index into ACTIVE status and makes it usable again for queries. \n",
        "\n",
        "The cell below shows an example of `restoreIndex` API. We delete \"deptIndex1\" and restore it. The output shows \"deptIndex1\" first went into the \"DELETED\" status after invoking \"deleteIndex\" command and came back to the \"ACTIVE\" status after calling \"restoreIndex\"."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "hyperspace.DeleteIndex(\"deptIndex1\");\n",
        "\n",
        "hyperspace.Indexes().Show();\n",
        "\n",
        "hyperspace.RestoreIndex(\"deptIndex1\");\n",
        "\n",
        "hyperspace.Indexes().Show();"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "medium",
              "session_id": 21,
              "statement_id": 10,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-03-15T07:01:20.6565515Z",
              "execution_start_time": "2021-03-15T07:02:48.0347456Z",
              "execution_finish_time": "2021-03-15T07:02:50.1026065Z"
            },
            "text/plain": "StatementMeta(medium, 21, 10, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 10,
          "data": {
            "text/plain": "+----------+--------------+---------------+----------+--------------------+--------------------+-------+\n|      name|indexedColumns|includedColumns|numBuckets|              schema|       indexLocation|  state|\n+----------+--------------+---------------+----------+--------------------+--------------------+-------+\n|deptIndex1|      [deptId]|     [deptName]|       200|{\"type\":\"struct\",...|abfss://default@h...|DELETED|\n|deptIndex2|    [location]|     [deptName]|       200|{\"type\":\"struct\",...|abfss://default@h...|DELETED|\n|  empIndex|      [deptId]|      [empName]|       200|{\"type\":\"struct\",...|abfss://default@h...| ACTIVE|\n+----------+--------------+---------------+----------+--------------------+--------------------+-------+\n\n+----------+--------------+---------------+----------+--------------------+--------------------+-------+\n|      name|indexedColumns|includedColumns|numBuckets|              schema|       indexLocation|  state|\n+----------+--------------+---------------+----------+--------------------+--------------------+-------+\n|deptIndex1|      [deptId]|     [deptName]|       200|{\"type\":\"struct\",...|abfss://default@h...| ACTIVE|\n|deptIndex2|    [location]|     [deptName]|       200|{\"type\":\"struct\",...|abfss://default@h...|DELETED|\n|  empIndex|      [deptId]|      [empName]|       200|{\"type\":\"struct\",...|abfss://default@h...| ACTIVE|\n+----------+--------------+---------------+----------+--------------------+--------------------+-------+\n\n"
          },
          "metadata": {}
        }
      ],
      "execution_count": 10,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vacuum Indexes\n",
        "The user can perform a **hard-delete** i.e., fully remove files and the metadata entry for a deleted index using the `vacuumIndex` API. Once done, this action is **irreversible** as it physically deletes all the index files associated with the index.\n",
        "\n",
        "The cell below vacuums the \"deptIndex2\" index and shows Hyperspace metadata after vaccuming. You should see metadata entries for two indexes \"deptIndex1\" and \"empIndex\" both with \"ACTIVE\" status and no entry for \"deptIndex2\"."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "hyperspace.VacuumIndex(\"deptIndex2\");\n",
        "\n",
        "hyperspace.Indexes().Show();"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "medium",
              "session_id": 21,
              "statement_id": 11,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-03-15T07:01:20.6583485Z",
              "execution_start_time": "2021-03-15T07:02:50.1876729Z",
              "execution_finish_time": "2021-03-15T07:02:52.2473038Z"
            },
            "text/plain": "StatementMeta(medium, 21, 11, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 11,
          "data": {
            "text/plain": "+----------+--------------+---------------+----------+--------------------+--------------------+------+\n|      name|indexedColumns|includedColumns|numBuckets|              schema|       indexLocation| state|\n+----------+--------------+---------------+----------+--------------------+--------------------+------+\n|deptIndex1|      [deptId]|     [deptName]|       200|{\"type\":\"struct\",...|abfss://default@h...|ACTIVE|\n|  empIndex|      [deptId]|      [empName]|       200|{\"type\":\"struct\",...|abfss://default@h...|ACTIVE|\n+----------+--------------+---------------+----------+--------------------+--------------------+------+\n\n"
          },
          "metadata": {}
        }
      ],
      "execution_count": 11,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Enable/Disable Hyperspace\n",
        "\n",
        "Hyperspace provides APIs to enable or disable index usage with Spark™.\n",
        "\n",
        "  - By using `enableHyperspace` API, Hyperspace optimization rules become visible to the Apache Spark™ optimizer and it will exploit existing Hyperspace indexes to optimize user queries.\n",
        "  - By using `disableHyperspace` command, Hyperspace rules no longer apply during query optimization. You should note that disabling Hyperspace has no impact on created indexes as they remain intact.\n",
        "\n",
        "Below cell shows how you can use these commands to enable or disable hyperspace. The output simply shows a reference to the existing Spark™ session whose configuration is updated."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "// Enable Hyperspace\n",
        "spark.EnableHyperspace();\n",
        "\n",
        "// Disable Hyperspace\n",
        "spark.DisableHyperspace();"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "medium",
              "session_id": 21,
              "statement_id": 12,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-03-15T07:01:20.6602277Z",
              "execution_start_time": "2021-03-15T07:02:52.3403996Z",
              "execution_finish_time": "2021-03-15T07:02:54.4039453Z"
            },
            "text/plain": "StatementMeta(medium, 21, 12, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 12,
          "data": {
            "text/plain": ""
          },
          "metadata": {}
        }
      ],
      "execution_count": 12,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Index Usage\n",
        "In order to make Spark™ use Hyperspace indexes during query processing, the user needs to make sure that Hyperspace is enabled. \n",
        "\n",
        "The cell below enables Hyperspace and creates two DataFrames containing our sample data records which we use for running example queries. For each DataFrame, a few sample rows are printed."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "// Enable Hyperspace\n",
        "spark.EnableHyperspace();\n",
        "\n",
        "DataFrame empDFrame = spark.Read().Parquet(empLocation);\n",
        "DataFrame deptDFrame = spark.Read().Parquet(deptLocation);\n",
        "\n",
        "empDFrame.Show(5);\n",
        "deptDFrame.Show(5);"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "medium",
              "session_id": 21,
              "statement_id": 13,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-03-15T07:01:20.6620357Z",
              "execution_start_time": "2021-03-15T07:02:54.491204Z",
              "execution_finish_time": "2021-03-15T07:02:56.5582441Z"
            },
            "text/plain": "StatementMeta(medium, 21, 13, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 13,
          "data": {
            "text/plain": "+-----+-------+------+\n|empId|empName|deptId|\n+-----+-------+------+\n| 7844| TURNER|    30|\n| 7934| MILLER|    10|\n| 7654| MARTIN|    30|\n| 7369|  SMITH|    20|\n| 7499|  ALLEN|    30|\n+-----+-------+------+\nonly showing top 5 rows\n\n+------+----------+--------+\n|deptId|  deptName|location|\n+------+----------+--------+\n|    10|Accounting|New York|\n|    40|Operations|  Boston|\n|    20|  Research|  Dallas|\n|    30|     Sales| Chicago|\n+------+----------+--------+\n\n"
          },
          "metadata": {}
        }
      ],
      "execution_count": 13,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperspace's Index Types\n",
        "\n",
        "Currently, Hyperspace can exploit indexes for two groups of queries: \n",
        "* Selection queries with lookup or range selection filtering predicates.\n",
        "* Join queries with an equality join predicate (i.e. Equi-joins)."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Indexes for Accelerating Filters\n",
        "\n",
        "Our first example query does a lookup on department records (see below cell). In SQL, this query looks as follows:\n",
        "\n",
        "```sql\n",
        "SELECT deptName \n",
        "FROM departments\n",
        "WHERE deptId = 20\n",
        "```\n",
        "\n",
        "The output of running the cell below shows: \n",
        "- query result, which is a single department name.\n",
        "- query plan that Spark™ used to run the query. \n",
        "\n",
        "In the query plan, the \"FileScan\" operator at the bottom of the plan shows the datasource where the records were read from. The location of this file indicates the path to the latest version of the \"deptIndex1\" index. This shows  that according to the query and using Hyperspace optimization rules, Spark™ decided to exploit the proper index at runtime.\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "// Filter with equality predicate\n",
        "DataFrame eqFilter = deptDFrame.Filter(\"deptId = 20\").Select(\"deptName\");\n",
        "eqFilter.Show();\n",
        "\n",
        "hyperspace.Explain(eqFilter, true, input => DisplayHTML(input));"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "medium",
              "session_id": 21,
              "statement_id": 14,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-03-15T07:01:20.6639149Z",
              "execution_start_time": "2021-03-15T07:02:56.6569642Z",
              "execution_finish_time": "2021-03-15T07:02:58.7199381Z"
            },
            "text/plain": "StatementMeta(medium, 21, 14, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "+--------+\n|deptName|\n+--------+\n|Research|\n+--------+\n\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": "<pre>=============================================================<br>Plan with indexes:<br>=============================================================<br>Project [deptName#513]<br>+- Filter (isnotnull(deptId#512) && (deptId#512 = 20))<br>   <b style=\"background:LightGreen\">+- FileScan Hyperspace(Type: CI, Name: deptIndex1, LogVersion: 5) [deptId#512,deptName#513] Batched: true, Format: Parquet, Location: InMemoryFileIndex[abfss://default@honghaigen2.dfs.core.windows.net/hyperspace/indexes-6785343/dep..., PartitionFilters: [], PushedFilters: [IsNotNull(deptId), EqualTo(deptId,20)], ReadSchema: struct<deptId:int,deptName:string></b><br><br>=============================================================<br>Plan without indexes:<br>=============================================================<br>Project [deptName#513]<br>+- Filter (isnotnull(deptId#512) && (deptId#512 = 20))<br>   <b style=\"background:LightGreen\">+- FileScan parquet [deptId#512,deptName#513] Batched: true, Format: Parquet, Location: InMemoryFileIndex[abfss://default@honghaigen2.dfs.core.windows.net/hyperspace/data-6785343/depart..., PartitionFilters: [], PushedFilters: [IsNotNull(deptId), EqualTo(deptId,20)], ReadSchema: struct<deptId:int,deptName:string></b><br><br>=============================================================<br>Indexes used:<br>=============================================================<br>deptIndex1:abfss://default@honghaigen2.dfs.core.windows.net/hyperspace/indexes-6785343/deptIndex1/v__=0<br><br>=============================================================<br>Physical operator stats:<br>=============================================================<br>+-----------------------------------------------------------+-------------------+------------------+----------+<br>|                                          Physical Operator|Hyperspace Disabled|Hyperspace Enabled|Difference|<br>+-----------------------------------------------------------+-------------------+------------------+----------+<br>|*Scan Hyperspace(Type: CI, Name: deptIndex1, LogVersion: 5)|                  0|                 1|         1|<br>|                                              *Scan parquet|                  1|                 0|        -1|<br>|                                                     Filter|                  1|                 1|         0|<br>|                                                    Project|                  1|                 1|         0|<br>|                                          WholeStageCodegen|                  1|                 1|         0|<br>+-----------------------------------------------------------+-------------------+------------------+----------+<br><br></pre>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "\n"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 14,
          "data": {},
          "metadata": {}
        }
      ],
      "execution_count": 14,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our second example is a range selection query on department records. In SQL, this query looks as follows:\n",
        "\n",
        "```sql\n",
        "SELECT deptName \n",
        "FROM departments\n",
        "WHERE deptId > 20\n",
        "```\n",
        "Similar to our first example, the output of the cell below shows the query results (names of two departments) and the query plan. The location of data file in the FileScan operator shows that 'deptIndex1\" was used to run the query.   \n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "// Filter with range selection predicate\n",
        "DataFrame rangeFilter = deptDFrame.Filter(\"deptId > 20\").Select(\"deptName\");\n",
        "rangeFilter.Show();\n",
        "\n",
        "hyperspace.Explain(rangeFilter, true, input => DisplayHTML(input));"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "medium",
              "session_id": 21,
              "statement_id": 15,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-03-15T07:01:27.3949247Z",
              "execution_start_time": "2021-03-15T07:02:58.8088597Z",
              "execution_finish_time": "2021-03-15T07:03:00.8783617Z"
            },
            "text/plain": "StatementMeta(medium, 21, 15, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "+----------+\n|  deptName|\n+----------+\n|Operations|\n|     Sales|\n+----------+\n\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": "<pre>=============================================================<br>Plan with indexes:<br>=============================================================<br>Project [deptName#513]<br>+- Filter (isnotnull(deptId#512) && (deptId#512 > 20))<br>   <b style=\"background:LightGreen\">+- FileScan Hyperspace(Type: CI, Name: deptIndex1, LogVersion: 5) [deptId#512,deptName#513] Batched: true, Format: Parquet, Location: InMemoryFileIndex[abfss://default@honghaigen2.dfs.core.windows.net/hyperspace/indexes-6785343/dep..., PartitionFilters: [], PushedFilters: [IsNotNull(deptId), GreaterThan(deptId,20)], ReadSchema: struct<deptId:int,deptName:string></b><br><br>=============================================================<br>Plan without indexes:<br>=============================================================<br>Project [deptName#513]<br>+- Filter (isnotnull(deptId#512) && (deptId#512 > 20))<br>   <b style=\"background:LightGreen\">+- FileScan parquet [deptId#512,deptName#513] Batched: true, Format: Parquet, Location: InMemoryFileIndex[abfss://default@honghaigen2.dfs.core.windows.net/hyperspace/data-6785343/depart..., PartitionFilters: [], PushedFilters: [IsNotNull(deptId), GreaterThan(deptId,20)], ReadSchema: struct<deptId:int,deptName:string></b><br><br>=============================================================<br>Indexes used:<br>=============================================================<br>deptIndex1:abfss://default@honghaigen2.dfs.core.windows.net/hyperspace/indexes-6785343/deptIndex1/v__=0<br><br>=============================================================<br>Physical operator stats:<br>=============================================================<br>+-----------------------------------------------------------+-------------------+------------------+----------+<br>|                                          Physical Operator|Hyperspace Disabled|Hyperspace Enabled|Difference|<br>+-----------------------------------------------------------+-------------------+------------------+----------+<br>|*Scan Hyperspace(Type: CI, Name: deptIndex1, LogVersion: 5)|                  0|                 1|         1|<br>|                                              *Scan parquet|                  1|                 0|        -1|<br>|                                                     Filter|                  1|                 1|         0|<br>|                                                    Project|                  1|                 1|         0|<br>|                                          WholeStageCodegen|                  1|                 1|         0|<br>+-----------------------------------------------------------+-------------------+------------------+----------+<br><br></pre>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "\n"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 15,
          "data": {},
          "metadata": {}
        }
      ],
      "execution_count": 15,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our third example is a query joining department and employee records on the department id. The equivalent SQL statement is shown below:\n",
        "\n",
        "```sql\n",
        "SELECT employees.deptId, empName, departments.deptId, deptName\n",
        "FROM   employees, departments \n",
        "WHERE  employees.deptId = departments.deptId\n",
        "```\n",
        "\n",
        "The output of running the cell below shows the query results which are the names of 14 employees and the name of department each employee works in. The query plan is also included in the output. Notice how the file locations for two FileScan operators shows that Spark™ used \"empIndex\" and \"deptIndex1\" indexes to run the query.   \n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "// Join\n",
        "DataFrame eqJoin =\n",
        "      empDFrame\n",
        "      .Join(deptDFrame, empDFrame.Col(\"deptId\") == deptDFrame.Col(\"deptId\"))\n",
        "      .Select(empDFrame.Col(\"empName\"), deptDFrame.Col(\"deptName\"));\n",
        "\n",
        "eqJoin.Show();\n",
        "\n",
        "hyperspace.Explain(eqJoin, true, input => DisplayHTML(input));"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "medium",
              "session_id": 21,
              "statement_id": 16,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-03-15T07:01:40.9647932Z",
              "execution_start_time": "2021-03-15T07:03:00.9708431Z",
              "execution_finish_time": "2021-03-15T07:03:03.0293903Z"
            },
            "text/plain": "StatementMeta(medium, 21, 16, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "+-------+----------+\n|empName|  deptName|\n+-------+----------+\n|  SCOTT|  Research|\n|  SMITH|  Research|\n|  JONES|  Research|\n|  ADAMS|  Research|\n|   FORD|  Research|\n| MILLER|Accounting|\n|  CLARK|Accounting|\n|   KING|Accounting|\n| TURNER|     Sales|\n| MARTIN|     Sales|\n|  ALLEN|     Sales|\n|  BLAKE|     Sales|\n|  JAMES|     Sales|\n|   WARD|     Sales|\n+-------+----------+\n\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": "<pre>=============================================================<br>Plan with indexes:<br>=============================================================<br>Project [empName#507, deptName#513]<br>+- SortMergeJoin [deptId#508], [deptId#512], Inner<br>   <b style=\"background:LightGreen\">:- *(1) Project [empName#507, deptId#508]</b><br>   <b style=\"background:LightGreen\">:  +- *(1) Filter isnotnull(deptId#508)</b><br>   <b style=\"background:LightGreen\">:     +- *(1) FileScan Hyperspace(Type: CI, Name: empIndex, LogVersion: 1) [deptId#508,empName#507] Batched: true, Format: Parquet, Location: InMemoryFileIndex[abfss://default@honghaigen2.dfs.core.windows.net/hyperspace/indexes-6785343/emp..., PartitionFilters: [], PushedFilters: [IsNotNull(deptId)], ReadSchema: struct<deptId:int,empName:string>, SelectedBucketsCount: 200 out of 200</b><br>   <b style=\"background:LightGreen\">+- *(2) Project [deptId#512, deptName#513]</b><br>      <b style=\"background:LightGreen\">+- *(2) Filter isnotnull(deptId#512)</b><br>         <b style=\"background:LightGreen\">+- *(2) FileScan Hyperspace(Type: CI, Name: deptIndex1, LogVersion: 5) [deptId#512,deptName#513] Batched: true, Format: Parquet, Location: InMemoryFileIndex[abfss://default@honghaigen2.dfs.core.windows.net/hyperspace/indexes-6785343/dep..., PartitionFilters: [], PushedFilters: [IsNotNull(deptId)], ReadSchema: struct<deptId:int,deptName:string>, SelectedBucketsCount: 200 out of 200</b><br><br>=============================================================<br>Plan without indexes:<br>=============================================================<br>Project [empName#507, deptName#513]<br>+- SortMergeJoin [deptId#508], [deptId#512], Inner<br>   <b style=\"background:LightGreen\">:- *(2) Sort [deptId#508 ASC NULLS FIRST], false, 0</b><br>   <b style=\"background:LightGreen\">:  +- Exchange hashpartitioning(deptId#508, 200), [id=#183]</b><br>   <b style=\"background:LightGreen\">:     +- *(1) Project [empName#507, deptId#508]</b><br>   <b style=\"background:LightGreen\">:        +- *(1) Filter isnotnull(deptId#508)</b><br>   <b style=\"background:LightGreen\">:           +- *(1) FileScan parquet [empName#507,deptId#508] Batched: true, Format: Parquet, Location: InMemoryFileIndex[abfss://default@honghaigen2.dfs.core.windows.net/hyperspace/data-6785343/employ..., PartitionFilters: [], PushedFilters: [IsNotNull(deptId)], ReadSchema: struct<empName:string,deptId:int></b><br>   <b style=\"background:LightGreen\">+- *(4) Sort [deptId#512 ASC NULLS FIRST], false, 0</b><br>      <b style=\"background:LightGreen\">+- Exchange hashpartitioning(deptId#512, 200), [id=#189]</b><br>         <b style=\"background:LightGreen\">+- *(3) Project [deptId#512, deptName#513]</b><br>            <b style=\"background:LightGreen\">+- *(3) Filter isnotnull(deptId#512)</b><br>               <b style=\"background:LightGreen\">+- *(3) FileScan parquet [deptId#512,deptName#513] Batched: true, Format: Parquet, Location: InMemoryFileIndex[abfss://default@honghaigen2.dfs.core.windows.net/hyperspace/data-6785343/depart..., PartitionFilters: [], PushedFilters: [IsNotNull(deptId)], ReadSchema: struct<deptId:int,deptName:string></b><br><br>=============================================================<br>Indexes used:<br>=============================================================<br>deptIndex1:abfss://default@honghaigen2.dfs.core.windows.net/hyperspace/indexes-6785343/deptIndex1/v__=0<br>empIndex:abfss://default@honghaigen2.dfs.core.windows.net/hyperspace/indexes-6785343/empIndex/v__=0<br><br>=============================================================<br>Physical operator stats:<br>=============================================================<br>+-----------------------------------------------------------+-------------------+------------------+----------+<br>|                                          Physical Operator|Hyperspace Disabled|Hyperspace Enabled|Difference|<br>+-----------------------------------------------------------+-------------------+------------------+----------+<br>|                                              *InputAdapter|                  4|                 2|        -2|<br>|*Scan Hyperspace(Type: CI, Name: deptIndex1, LogVersion: 5)|                  0|                 1|         1|<br>|  *Scan Hyperspace(Type: CI, Name: empIndex, LogVersion: 1)|                  0|                 1|         1|<br>|                                              *Scan parquet|                  2|                 0|        -2|<br>|                                           *ShuffleExchange|                  2|                 0|        -2|<br>|                                                      *Sort|                  2|                 0|        -2|<br>|                                         *WholeStageCodegen|                  5|                 3|        -2|<br>|                                                     Filter|                  2|                 2|         0|<br>|                                                    Project|                  3|                 3|         0|<br>|                                              SortMergeJoin|                  1|                 1|         0|<br>+-----------------------------------------------------------+-------------------+------------------+----------+<br><br></pre>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "\n"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 16,
          "data": {},
          "metadata": {}
        }
      ],
      "execution_count": 16,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Support for SQL Semantics\n",
        "\n",
        "The index usage is transparent to whether the user uses DataFrame API or Spark™ SQL. The following example shows the same join example as before but using Spark SQL, showing the use of indexes if applicable."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "empDFrame.CreateOrReplaceTempView(\"EMP\");\n",
        "deptDFrame.CreateOrReplaceTempView(\"DEPT\");\n",
        "\n",
        "var joinQuery = spark.Sql(\"SELECT EMP.empName, DEPT.deptName FROM EMP, DEPT WHERE EMP.deptId = DEPT.deptId\");\n",
        "\n",
        "joinQuery.Show();\n",
        "hyperspace.Explain(joinQuery, true, input => DisplayHTML(input));"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "medium",
              "session_id": 21,
              "statement_id": 17,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-03-15T07:01:41.6834567Z",
              "execution_start_time": "2021-03-15T07:03:03.1226671Z",
              "execution_finish_time": "2021-03-15T07:03:09.3062542Z"
            },
            "text/plain": "StatementMeta(medium, 21, 17, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "+-------+----------+\n|empName|  deptName|\n+-------+----------+\n|  SCOTT|  Research|\n|  SMITH|  Research|\n|  JONES|  Research|\n|  ADAMS|  Research|\n|   FORD|  Research|\n| MILLER|Accounting|\n|  CLARK|Accounting|\n|   KING|Accounting|\n| TURNER|     Sales|\n| MARTIN|     Sales|\n|  ALLEN|     Sales|\n|  BLAKE|     Sales|\n|  JAMES|     Sales|\n|   WARD|     Sales|\n+-------+----------+\n\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": "<pre>=============================================================<br>Plan with indexes:<br>=============================================================<br>Project [empName#507, deptName#513]<br>+- SortMergeJoin [deptId#508], [deptId#512], Inner<br>   <b style=\"background:LightGreen\">:- *(1) Project [empName#507, deptId#508]</b><br>   <b style=\"background:LightGreen\">:  +- *(1) Filter isnotnull(deptId#508)</b><br>   <b style=\"background:LightGreen\">:     +- *(1) FileScan Hyperspace(Type: CI, Name: empIndex, LogVersion: 1) [deptId#508,empName#507] Batched: true, Format: Parquet, Location: InMemoryFileIndex[abfss://default@honghaigen2.dfs.core.windows.net/hyperspace/indexes-6785343/emp..., PartitionFilters: [], PushedFilters: [IsNotNull(deptId)], ReadSchema: struct<deptId:int,empName:string>, SelectedBucketsCount: 200 out of 200</b><br>   <b style=\"background:LightGreen\">+- *(2) Project [deptId#512, deptName#513]</b><br>      <b style=\"background:LightGreen\">+- *(2) Filter isnotnull(deptId#512)</b><br>         <b style=\"background:LightGreen\">+- *(2) FileScan Hyperspace(Type: CI, Name: deptIndex1, LogVersion: 5) [deptId#512,deptName#513] Batched: true, Format: Parquet, Location: InMemoryFileIndex[abfss://default@honghaigen2.dfs.core.windows.net/hyperspace/indexes-6785343/dep..., PartitionFilters: [], PushedFilters: [IsNotNull(deptId)], ReadSchema: struct<deptId:int,deptName:string>, SelectedBucketsCount: 200 out of 200</b><br><br>=============================================================<br>Plan without indexes:<br>=============================================================<br>Project [empName#507, deptName#513]<br>+- SortMergeJoin [deptId#508], [deptId#512], Inner<br>   <b style=\"background:LightGreen\">:- *(2) Sort [deptId#508 ASC NULLS FIRST], false, 0</b><br>   <b style=\"background:LightGreen\">:  +- Exchange hashpartitioning(deptId#508, 200), [id=#279]</b><br>   <b style=\"background:LightGreen\">:     +- *(1) Project [empName#507, deptId#508]</b><br>   <b style=\"background:LightGreen\">:        +- *(1) Filter isnotnull(deptId#508)</b><br>   <b style=\"background:LightGreen\">:           +- *(1) FileScan parquet [empName#507,deptId#508] Batched: true, Format: Parquet, Location: InMemoryFileIndex[abfss://default@honghaigen2.dfs.core.windows.net/hyperspace/data-6785343/employ..., PartitionFilters: [], PushedFilters: [IsNotNull(deptId)], ReadSchema: struct<empName:string,deptId:int></b><br>   <b style=\"background:LightGreen\">+- *(4) Sort [deptId#512 ASC NULLS FIRST], false, 0</b><br>      <b style=\"background:LightGreen\">+- Exchange hashpartitioning(deptId#512, 200), [id=#285]</b><br>         <b style=\"background:LightGreen\">+- *(3) Project [deptId#512, deptName#513]</b><br>            <b style=\"background:LightGreen\">+- *(3) Filter isnotnull(deptId#512)</b><br>               <b style=\"background:LightGreen\">+- *(3) FileScan parquet [deptId#512,deptName#513] Batched: true, Format: Parquet, Location: InMemoryFileIndex[abfss://default@honghaigen2.dfs.core.windows.net/hyperspace/data-6785343/depart..., PartitionFilters: [], PushedFilters: [IsNotNull(deptId)], ReadSchema: struct<deptId:int,deptName:string></b><br><br>=============================================================<br>Indexes used:<br>=============================================================<br>deptIndex1:abfss://default@honghaigen2.dfs.core.windows.net/hyperspace/indexes-6785343/deptIndex1/v__=0<br>empIndex:abfss://default@honghaigen2.dfs.core.windows.net/hyperspace/indexes-6785343/empIndex/v__=0<br><br>=============================================================<br>Physical operator stats:<br>=============================================================<br>+-----------------------------------------------------------+-------------------+------------------+----------+<br>|                                          Physical Operator|Hyperspace Disabled|Hyperspace Enabled|Difference|<br>+-----------------------------------------------------------+-------------------+------------------+----------+<br>|                                              *InputAdapter|                  4|                 2|        -2|<br>|*Scan Hyperspace(Type: CI, Name: deptIndex1, LogVersion: 5)|                  0|                 1|         1|<br>|  *Scan Hyperspace(Type: CI, Name: empIndex, LogVersion: 1)|                  0|                 1|         1|<br>|                                              *Scan parquet|                  2|                 0|        -2|<br>|                                           *ShuffleExchange|                  2|                 0|        -2|<br>|                                                      *Sort|                  2|                 0|        -2|<br>|                                         *WholeStageCodegen|                  5|                 3|        -2|<br>|                                                     Filter|                  2|                 2|         0|<br>|                                                    Project|                  3|                 3|         0|<br>|                                              SortMergeJoin|                  1|                 1|         0|<br>+-----------------------------------------------------------+-------------------+------------------+----------+<br><br></pre>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "\n"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 17,
          "data": {},
          "metadata": {}
        }
      ],
      "execution_count": 17,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Explain API\n",
        "\n",
        "So far, you might have observed we have been using the explain API provided by Hyperspace. The `explain` API from Hyperspace is very similar to Spark's `df.explain` API but allows users to compare their original plan vs the updated index-dependent plan before running their query. You have an option to choose from html/plaintext/console mode to display the command output. \n",
        "\n",
        "The following cell shows an example with HTML. The highlighted section represents the difference between original and updated plans along with the indexes being used."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "spark.Conf().Set(\"spark.hyperspace.explain.displayMode\", \"html\");\n",
        "spark.Conf().Set(\"spark.hyperspace.explain.displayMode.highlight.beginTag\", \"<b style=\\\"background:LightGreen\\\">\");\n",
        "spark.Conf().Set(\"spark.hyperspace.explain.displayMode.highlight.endTag\", \"</b>\");\n",
        "\n",
        "hyperspace.Explain(eqJoin, true, input => DisplayHTML(input));"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "medium",
              "session_id": 21,
              "statement_id": 18,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-03-15T07:01:41.8390732Z",
              "execution_start_time": "2021-03-15T07:03:09.3970877Z",
              "execution_finish_time": "2021-03-15T07:03:11.4514895Z"
            },
            "text/plain": "StatementMeta(medium, 21, 18, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": "<pre>=============================================================<br>Plan with indexes:<br>=============================================================<br>Project [empName#507, deptName#513]<br>+- SortMergeJoin [deptId#508], [deptId#512], Inner<br>   <b style=\"background:LightGreen\">:- *(1) Project [empName#507, deptId#508]</b><br>   <b style=\"background:LightGreen\">:  +- *(1) Filter isnotnull(deptId#508)</b><br>   <b style=\"background:LightGreen\">:     +- *(1) FileScan Hyperspace(Type: CI, Name: empIndex, LogVersion: 1) [deptId#508,empName#507] Batched: true, Format: Parquet, Location: InMemoryFileIndex[abfss://default@honghaigen2.dfs.core.windows.net/hyperspace/indexes-6785343/emp..., PartitionFilters: [], PushedFilters: [IsNotNull(deptId)], ReadSchema: struct<deptId:int,empName:string>, SelectedBucketsCount: 200 out of 200</b><br>   <b style=\"background:LightGreen\">+- *(2) Project [deptId#512, deptName#513]</b><br>      <b style=\"background:LightGreen\">+- *(2) Filter isnotnull(deptId#512)</b><br>         <b style=\"background:LightGreen\">+- *(2) FileScan Hyperspace(Type: CI, Name: deptIndex1, LogVersion: 5) [deptId#512,deptName#513] Batched: true, Format: Parquet, Location: InMemoryFileIndex[abfss://default@honghaigen2.dfs.core.windows.net/hyperspace/indexes-6785343/dep..., PartitionFilters: [], PushedFilters: [IsNotNull(deptId)], ReadSchema: struct<deptId:int,deptName:string>, SelectedBucketsCount: 200 out of 200</b><br><br>=============================================================<br>Plan without indexes:<br>=============================================================<br>Project [empName#507, deptName#513]<br>+- SortMergeJoin [deptId#508], [deptId#512], Inner<br>   <b style=\"background:LightGreen\">:- *(2) Sort [deptId#508 ASC NULLS FIRST], false, 0</b><br>   <b style=\"background:LightGreen\">:  +- Exchange hashpartitioning(deptId#508, 200), [id=#345]</b><br>   <b style=\"background:LightGreen\">:     +- *(1) Project [empName#507, deptId#508]</b><br>   <b style=\"background:LightGreen\">:        +- *(1) Filter isnotnull(deptId#508)</b><br>   <b style=\"background:LightGreen\">:           +- *(1) FileScan parquet [empName#507,deptId#508] Batched: true, Format: Parquet, Location: InMemoryFileIndex[abfss://default@honghaigen2.dfs.core.windows.net/hyperspace/data-6785343/employ..., PartitionFilters: [], PushedFilters: [IsNotNull(deptId)], ReadSchema: struct<empName:string,deptId:int></b><br>   <b style=\"background:LightGreen\">+- *(4) Sort [deptId#512 ASC NULLS FIRST], false, 0</b><br>      <b style=\"background:LightGreen\">+- Exchange hashpartitioning(deptId#512, 200), [id=#351]</b><br>         <b style=\"background:LightGreen\">+- *(3) Project [deptId#512, deptName#513]</b><br>            <b style=\"background:LightGreen\">+- *(3) Filter isnotnull(deptId#512)</b><br>               <b style=\"background:LightGreen\">+- *(3) FileScan parquet [deptId#512,deptName#513] Batched: true, Format: Parquet, Location: InMemoryFileIndex[abfss://default@honghaigen2.dfs.core.windows.net/hyperspace/data-6785343/depart..., PartitionFilters: [], PushedFilters: [IsNotNull(deptId)], ReadSchema: struct<deptId:int,deptName:string></b><br><br>=============================================================<br>Indexes used:<br>=============================================================<br>deptIndex1:abfss://default@honghaigen2.dfs.core.windows.net/hyperspace/indexes-6785343/deptIndex1/v__=0<br>empIndex:abfss://default@honghaigen2.dfs.core.windows.net/hyperspace/indexes-6785343/empIndex/v__=0<br><br>=============================================================<br>Physical operator stats:<br>=============================================================<br>+-----------------------------------------------------------+-------------------+------------------+----------+<br>|                                          Physical Operator|Hyperspace Disabled|Hyperspace Enabled|Difference|<br>+-----------------------------------------------------------+-------------------+------------------+----------+<br>|                                              *InputAdapter|                  4|                 2|        -2|<br>|*Scan Hyperspace(Type: CI, Name: deptIndex1, LogVersion: 5)|                  0|                 1|         1|<br>|  *Scan Hyperspace(Type: CI, Name: empIndex, LogVersion: 1)|                  0|                 1|         1|<br>|                                              *Scan parquet|                  2|                 0|        -2|<br>|                                           *ShuffleExchange|                  2|                 0|        -2|<br>|                                                      *Sort|                  2|                 0|        -2|<br>|                                         *WholeStageCodegen|                  5|                 3|        -2|<br>|                                                     Filter|                  2|                 2|         0|<br>|                                                    Project|                  3|                 3|         0|<br>|                                              SortMergeJoin|                  1|                 1|         0|<br>+-----------------------------------------------------------+-------------------+------------------+----------+<br><br></pre>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "\n"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 18,
          "data": {},
          "metadata": {}
        }
      ],
      "execution_count": 18,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Refresh Indexes\n",
        "If the original data on which an index was created changes, then the index will no longer capture the latest state of data and hence will not be used by Hyperspace to provide any acceleration. The user can refresh such a stale index using the `refreshIndex` API. This causes the index to be fully rebuilt and updates it according to the latest data records.\n",
        "    \n",
        "    Spoiler alert: if you are worried about fully rebuilding your index every time your data changes, don't worry! We will show you how to *incrementally refresh* your index in subsequent cells below.\n",
        "\n",
        "The two cells below show an example for this scenario:\n",
        "- First cell adds two more departments to the original departments data. It reads and prints list of departments to verify new departments are added correctly. The output shows 6 departments in total: four old ones and two new. Invoking \"refreshIndex\" updates \"deptIndex1\" so index captures new departments.\n",
        "- Second cell runs our range selection query example. The results should now contain four departments: two are the ones, seen before when we ran the query above, and two are the new departments we just added."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "var extraDepartments = new List<GenericRow>()\n",
        "{\n",
        "    new GenericRow(new object[] {50, \"Inovation\", \"Seattle\"}),\n",
        "    new GenericRow(new object[] {60, \"Human Resources\", \"San Francisco\"})\n",
        "};\n",
        "\t  \n",
        "DataFrame extraDeptData = spark.CreateDataFrame(extraDepartments, departmentSchema);\n",
        "extraDeptData.Write().Mode(\"Append\").Parquet(deptLocation);\n",
        "\n",
        "DataFrame deptDFrameUpdated = spark.Read().Parquet(deptLocation);\n",
        "\n",
        "deptDFrameUpdated.Show(10);\n",
        "\n",
        "hyperspace.RefreshIndex(\"deptIndex1\");"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "medium",
              "session_id": 21,
              "statement_id": 19,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-03-15T07:01:41.9529821Z",
              "execution_start_time": "2021-03-15T07:03:11.5397789Z",
              "execution_finish_time": "2021-03-15T07:03:15.713973Z"
            },
            "text/plain": "StatementMeta(medium, 21, 19, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 19,
          "data": {
            "text/plain": "+------+---------------+-------------+\n|deptId|       deptName|     location|\n+------+---------------+-------------+\n|    60|Human Resources|San Francisco|\n|    10|     Accounting|     New York|\n|    50|      Inovation|      Seattle|\n|    40|     Operations|       Boston|\n|    20|       Research|       Dallas|\n|    30|          Sales|      Chicago|\n+------+---------------+-------------+\n\n"
          },
          "metadata": {}
        }
      ],
      "execution_count": 19,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "DataFrame newRangeFilter = deptDFrameUpdated.Filter(\"deptId > 20\").Select(\"deptName\");\n",
        "newRangeFilter.Show();\n",
        "\n",
        "hyperspace.Explain(newRangeFilter, true, input => DisplayHTML(input));"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "medium",
              "session_id": 21,
              "statement_id": 20,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-03-15T07:01:42.5297707Z",
              "execution_start_time": "2021-03-15T07:03:15.7997581Z",
              "execution_finish_time": "2021-03-15T07:03:17.8597043Z"
            },
            "text/plain": "StatementMeta(medium, 21, 20, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "+---------------+\n|       deptName|\n+---------------+\n|Human Resources|\n|      Inovation|\n|     Operations|\n|          Sales|\n+---------------+\n\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": "<pre>=============================================================<br>Plan with indexes:<br>=============================================================<br>Project [deptName#1083]<br>+- Filter (isnotnull(deptId#1082) && (deptId#1082 > 20))<br>   <b style=\"background:LightGreen\">+- FileScan Hyperspace(Type: CI, Name: deptIndex1, LogVersion: 7) [deptId#1082,deptName#1083] Batched: true, Format: Parquet, Location: InMemoryFileIndex[abfss://default@honghaigen2.dfs.core.windows.net/hyperspace/indexes-6785343/dep..., PartitionFilters: [], PushedFilters: [IsNotNull(deptId), GreaterThan(deptId,20)], ReadSchema: struct<deptId:int,deptName:string></b><br><br>=============================================================<br>Plan without indexes:<br>=============================================================<br>Project [deptName#1083]<br>+- Filter (isnotnull(deptId#1082) && (deptId#1082 > 20))<br>   <b style=\"background:LightGreen\">+- FileScan parquet [deptId#1082,deptName#1083] Batched: true, Format: Parquet, Location: InMemoryFileIndex[abfss://default@honghaigen2.dfs.core.windows.net/hyperspace/data-6785343/depart..., PartitionFilters: [], PushedFilters: [IsNotNull(deptId), GreaterThan(deptId,20)], ReadSchema: struct<deptId:int,deptName:string></b><br><br>=============================================================<br>Indexes used:<br>=============================================================<br>deptIndex1:abfss://default@honghaigen2.dfs.core.windows.net/hyperspace/indexes-6785343/deptIndex1/v__=1<br><br>=============================================================<br>Physical operator stats:<br>=============================================================<br>+-----------------------------------------------------------+-------------------+------------------+----------+<br>|                                          Physical Operator|Hyperspace Disabled|Hyperspace Enabled|Difference|<br>+-----------------------------------------------------------+-------------------+------------------+----------+<br>|*Scan Hyperspace(Type: CI, Name: deptIndex1, LogVersion: 7)|                  0|                 1|         1|<br>|                                              *Scan parquet|                  1|                 0|        -1|<br>|                                                     Filter|                  1|                 1|         0|<br>|                                                    Project|                  1|                 1|         0|<br>|                                          WholeStageCodegen|                  1|                 1|         0|<br>+-----------------------------------------------------------+-------------------+------------------+----------+<br><br></pre>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "\n"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 20,
          "data": {},
          "metadata": {}
        }
      ],
      "execution_count": 20,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hybrid Scan for Mutable Datasets\n",
        "\n",
        "Often times, if your underlying source data had some new files appended or existing files deleted, your index will get stale and Hyperspace decides not to use it. However, there are times where you just want to use the index without having to refresh it everytime. There could be multiple reasons for doing so:\n",
        "\n",
        "  1. You do not want to continuosly refresh your index but instead want to do it periodically since you understand your workloads the best.  \n",
        "  2. You added/removed only a few files and do not want to wait for yet another refresh job to finish. \n",
        "\n",
        "To allow you to still use a stale index, Hyperspace introduces **Hybrid Scan**, a novel technique that allows users to utilize outdated or stale indexes (e.g., the underlying source data had some new files appended or existing files deleted), without refreshing indexes. \n",
        "\n",
        "To achieve this, when you set the appropriate configuration to enable Hybrid Scan, Hyperspace modifies the query plan to leverage the changes as following:\n",
        "- Appended files can be merged to index data by using **`Union` or `BucketUnion` (for join)**. Shuffling appended data can also be applied before merging, if needed.\n",
        "- Deleted files can be handled by injecting `Filter-NOT-IN` condition on **lineage column** of index data, so that the indexed rows from the deleted files can be excluded at query time. \n",
        "\n",
        "You can check the transformation of the query plan in below examples.\n",
        "\n",
        "    Note: Hybrid scan is only supported for non-partitioned data. Support for partitioned data is currently being worked upon."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hybrid Scan for appended files - non-partitioned data\n",
        "\n",
        "Non-partitioned data is used in below example. In this example, we expect Join index can be used for the query and `BucketUnion` is introduced for appended files."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "// GENERATE TEST DATA\n",
        "using Microsoft.Spark.Sql.Types;\n",
        "\n",
        "var products = new List<GenericRow>() {\n",
        "    new GenericRow(new object[] {\"orange\", 3, \"2020-10-01\"}),\n",
        "    new GenericRow(new object[] {\"banana\", 1, \"2020-10-01\"}),\n",
        "    new GenericRow(new object[] {\"carrot\", 5, \"2020-10-02\"}),\n",
        "    new GenericRow(new object[] {\"beetroot\", 12, \"2020-10-02\"}),\n",
        "    new GenericRow(new object[] {\"orange\", 2, \"2020-10-03\"}),\n",
        "    new GenericRow(new object[] {\"banana\", 11, \"2020-10-03\"}),\n",
        "    new GenericRow(new object[] {\"carrot\", 3, \"2020-10-03\"}),\n",
        "    new GenericRow(new object[] {\"beetroot\", 2, \"2020-10-04\"}),\n",
        "    new GenericRow(new object[] {\"cucumber\", 7, \"2020-10-05\"}),\n",
        "    new GenericRow(new object[] {\"pepper\", 20, \"2020-10-06\"})\n",
        "};\n",
        "var productsSchema = new StructType(new List<StructField>()\n",
        "{\n",
        "    new StructField(\"name\", new StringType()),\n",
        "    new StructField(\"qty\", new IntegerType()),\n",
        "    new StructField(\"date\", new StringType())\n",
        "});\n",
        "\n",
        "DataFrame testData = spark.CreateDataFrame(products, productsSchema); \n",
        "string testDataLocation = $\"{dataPath}/productTable\";\n",
        "testData.Write().Mode(\"overwrite\").Parquet(testDataLocation);"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "medium",
              "session_id": 21,
              "statement_id": 21,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-03-15T07:01:43.2555588Z",
              "execution_start_time": "2021-03-15T07:03:17.9579223Z",
              "execution_finish_time": "2021-03-15T07:03:20.0328276Z"
            },
            "text/plain": "StatementMeta(medium, 21, 21, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 21,
          "data": {
            "text/plain": ""
          },
          "metadata": {}
        }
      ],
      "execution_count": 21,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "// CREATE INDEX\n",
        "DataFrame testDF = spark.Read().Parquet(testDataLocation);\n",
        "var productIndex2Config = new IndexConfig(\"productIndex\", new string[] {\"name\"}, new string[] {\"date\", \"qty\"});\n",
        "hyperspace.CreateIndex(testDF, productIndex2Config);"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "medium",
              "session_id": 21,
              "statement_id": 22,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-03-15T07:01:44.2478635Z",
              "execution_start_time": "2021-03-15T07:03:20.1163368Z",
              "execution_finish_time": "2021-03-15T07:03:24.232053Z"
            },
            "text/plain": "StatementMeta(medium, 21, 22, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 22,
          "data": {
            "text/plain": ""
          },
          "metadata": {}
        }
      ],
      "execution_count": 22,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "DataFrame filter1 = testDF.Filter(\"name = 'banana'\");\n",
        "DataFrame filter2 = testDF.Filter(\"qty > 10\");\n",
        "DataFrame query = filter1.Join(filter2, filter1.Col(\"name\") == filter2.Col(\"name\"));\n",
        "\n",
        "query.Show();\n",
        "\n",
        "hyperspace.Explain(query, true, input => DisplayHTML(input));"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "medium",
              "session_id": 21,
              "statement_id": 23,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-03-15T07:01:45.2415727Z",
              "execution_start_time": "2021-03-15T07:03:24.3156962Z",
              "execution_finish_time": "2021-03-15T07:03:26.3829799Z"
            },
            "text/plain": "StatementMeta(medium, 21, 23, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "+------+---+----------+------+---+----------+\n|  name|qty|      date|  name|qty|      date|\n+------+---+----------+------+---+----------+\n|banana| 11|2020-10-03|banana| 11|2020-10-03|\n|banana|  1|2020-10-01|banana| 11|2020-10-03|\n+------+---+----------+------+---+----------+\n\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": "<pre>=============================================================<br>Plan with indexes:<br>=============================================================<br>SortMergeJoin [name#1225], [name#1249], Inner<br><b style=\"background:LightGreen\">:- *(1) Project [name#1225, qty#1226, date#1227]</b><br><b style=\"background:LightGreen\">:  +- *(1) Filter (isnotnull(name#1225) && (name#1225 = banana))</b><br><b style=\"background:LightGreen\">:     +- *(1) FileScan Hyperspace(Type: CI, Name: productIndex, LogVersion: 1) [name#1225,date#1227,qty#1226] Batched: true, Format: Parquet, Location: InMemoryFileIndex[abfss://default@honghaigen2.dfs.core.windows.net/hyperspace/indexes-6785343/pro..., PartitionFilters: [], PushedFilters: [IsNotNull(name), EqualTo(name,banana)], ReadSchema: struct<name:string,date:string,qty:int>, SelectedBucketsCount: 1 out of 200</b><br><b style=\"background:LightGreen\">+- *(2) Project [name#1249, qty#1250, date#1251]</b><br>   <b style=\"background:LightGreen\">+- *(2) Filter (((isnotnull(qty#1250) && (qty#1250 > 10)) && isnotnull(name#1249)) && (name#1249 = banana))</b><br>      <b style=\"background:LightGreen\">+- *(2) FileScan Hyperspace(Type: CI, Name: productIndex, LogVersion: 1) [name#1249,date#1251,qty#1250] Batched: true, Format: Parquet, Location: InMemoryFileIndex[abfss://default@honghaigen2.dfs.core.windows.net/hyperspace/indexes-6785343/pro..., PartitionFilters: [], PushedFilters: [IsNotNull(qty), GreaterThan(qty,10), IsNotNull(name), EqualTo(name,banana)], ReadSchema: struct<name:string,date:string,qty:int>, SelectedBucketsCount: 1 out of 200</b><br><br>=============================================================<br>Plan without indexes:<br>=============================================================<br>SortMergeJoin [name#1225], [name#1249], Inner<br><b style=\"background:LightGreen\">:- *(2) Sort [name#1225 ASC NULLS FIRST], false, 0</b><br><b style=\"background:LightGreen\">:  +- Exchange hashpartitioning(name#1225, 200), [id=#499]</b><br><b style=\"background:LightGreen\">:     +- *(1) Project [name#1225, qty#1226, date#1227]</b><br><b style=\"background:LightGreen\">:        +- *(1) Filter (isnotnull(name#1225) && (name#1225 = banana))</b><br><b style=\"background:LightGreen\">:           +- *(1) FileScan parquet [name#1225,qty#1226,date#1227] Batched: true, Format: Parquet, Location: InMemoryFileIndex[abfss://default@honghaigen2.dfs.core.windows.net/hyperspace/data-6785343/produc..., PartitionFilters: [], PushedFilters: [IsNotNull(name), EqualTo(name,banana)], ReadSchema: struct<name:string,qty:int,date:string></b><br><b style=\"background:LightGreen\">+- *(4) Sort [name#1249 ASC NULLS FIRST], false, 0</b><br>   <b style=\"background:LightGreen\">+- Exchange hashpartitioning(name#1249, 200), [id=#505]</b><br>      <b style=\"background:LightGreen\">+- *(3) Project [name#1249, qty#1250, date#1251]</b><br>         <b style=\"background:LightGreen\">+- *(3) Filter (((isnotnull(qty#1250) && (qty#1250 > 10)) && isnotnull(name#1249)) && (name#1249 = banana))</b><br>            <b style=\"background:LightGreen\">+- *(3) FileScan parquet [name#1249,qty#1250,date#1251] Batched: true, Format: Parquet, Location: InMemoryFileIndex[abfss://default@honghaigen2.dfs.core.windows.net/hyperspace/data-6785343/produc..., PartitionFilters: [], PushedFilters: [IsNotNull(qty), GreaterThan(qty,10), IsNotNull(name), EqualTo(name,banana)], ReadSchema: struct<name:string,qty:int,date:string></b><br><br>=============================================================<br>Indexes used:<br>=============================================================<br>productIndex:abfss://default@honghaigen2.dfs.core.windows.net/hyperspace/indexes-6785343/productIndex/v__=0<br><br>=============================================================<br>Physical operator stats:<br>=============================================================<br>+-------------------------------------------------------------+-------------------+------------------+----------+<br>|                                            Physical Operator|Hyperspace Disabled|Hyperspace Enabled|Difference|<br>+-------------------------------------------------------------+-------------------+------------------+----------+<br>|                                                *InputAdapter|                  4|                 2|        -2|<br>|*Scan Hyperspace(Type: CI, Name: productIndex, LogVersion: 1)|                  0|                 2|         2|<br>|                                                *Scan parquet|                  2|                 0|        -2|<br>|                                             *ShuffleExchange|                  2|                 0|        -2|<br>|                                                        *Sort|                  2|                 0|        -2|<br>|                                           *WholeStageCodegen|                  5|                 3|        -2|<br>|                                                       Filter|                  2|                 2|         0|<br>|                                                      Project|                  2|                 2|         0|<br>|                                                SortMergeJoin|                  1|                 1|         0|<br>+-------------------------------------------------------------+-------------------+------------------+----------+<br><br></pre>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "\n"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 23,
          "data": {},
          "metadata": {}
        }
      ],
      "execution_count": 23,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "// Append new files.\n",
        "var appendProducts = new List<GenericRow>()\n",
        "{\n",
        "    new GenericRow(new object[] {\"orange\", 13, \"2020-11-01\"}),\n",
        "    new GenericRow(new object[] {\"banana\", 5, \"2020-11-01\"})\n",
        "};\n",
        "    \n",
        "DataFrame appendData = spark.CreateDataFrame(appendProducts, productsSchema);\n",
        "appendData.Write().Mode(\"Append\").Parquet(testDataLocation);"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "medium",
              "session_id": 21,
              "statement_id": 24,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-03-15T07:01:46.2447559Z",
              "execution_start_time": "2021-03-15T07:03:26.4736389Z",
              "execution_finish_time": "2021-03-15T07:03:28.5362939Z"
            },
            "text/plain": "StatementMeta(medium, 21, 24, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 24,
          "data": {
            "text/plain": ""
          },
          "metadata": {}
        }
      ],
      "execution_count": 24,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hybrid scan is disabled by default. Therefore, you will see that since we appended new data, Hyperspace will decide NOT to use the index.\n",
        "\n",
        "In the output, you will see no plan differences (hence no highlighting)."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "// Hybrid Scan configs are false by default.\n",
        "spark.Conf().Set(\"spark.hyperspace.index.hybridscan.enabled\", \"false\");\n",
        "spark.Conf().Set(\"spark.hyperspace.index.hybridscan.delete.enabled\", \"false\");\n",
        "\n",
        "DataFrame testDFWithAppend = spark.Read().Parquet(testDataLocation);\n",
        "DataFrame filter1 = testDFWithAppend.Filter(\"name = 'banana'\");\n",
        "DataFrame filter2 = testDFWithAppend.Filter(\"qty > 10\");\n",
        "DataFrame query = filter1.Join(filter2, filter1.Col(\"name\") == filter2.Col(\"name\"));\n",
        "\n",
        "query.Show();\n",
        "\n",
        "hyperspace.Explain(query, true, input => DisplayHTML(input));"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "medium",
              "session_id": 21,
              "statement_id": 25,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-03-15T07:01:47.2501633Z",
              "execution_start_time": "2021-03-15T07:03:28.6181836Z",
              "execution_finish_time": "2021-03-15T07:03:30.6837284Z"
            },
            "text/plain": "StatementMeta(medium, 21, 25, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "+------+---+----------+------+---+----------+\n|  name|qty|      date|  name|qty|      date|\n+------+---+----------+------+---+----------+\n|banana|  1|2020-10-01|banana| 11|2020-10-03|\n|banana|  5|2020-11-01|banana| 11|2020-10-03|\n|banana| 11|2020-10-03|banana| 11|2020-10-03|\n+------+---+----------+------+---+----------+\n\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": "<pre>=============================================================<br>Plan with indexes:<br>=============================================================<br>SortMergeJoin [name#1408], [name#1414], Inner<br>:- *(2) Sort [name#1408 ASC NULLS FIRST], false, 0<br>:  +- Exchange hashpartitioning(name#1408, 200), [id=#682]<br>:     +- *(1) Project [name#1408, qty#1409, date#1410]<br>:        +- *(1) Filter (isnotnull(name#1408) && (name#1408 = banana))<br>:           +- *(1) FileScan parquet [name#1408,qty#1409,date#1410] Batched: true, Format: Parquet, Location: InMemoryFileIndex[abfss://default@honghaigen2.dfs.core.windows.net/hyperspace/data-6785343/produc..., PartitionFilters: [], PushedFilters: [IsNotNull(name), EqualTo(name,banana)], ReadSchema: struct<name:string,qty:int,date:string><br>+- *(4) Sort [name#1414 ASC NULLS FIRST], false, 0<br>   +- Exchange hashpartitioning(name#1414, 200), [id=#688]<br>      +- *(3) Project [name#1414, qty#1415, date#1416]<br>         +- *(3) Filter (((isnotnull(qty#1415) && (qty#1415 > 10)) && (name#1414 = banana)) && isnotnull(name#1414))<br>            +- *(3) FileScan parquet [name#1414,qty#1415,date#1416] Batched: true, Format: Parquet, Location: InMemoryFileIndex[abfss://default@honghaigen2.dfs.core.windows.net/hyperspace/data-6785343/produc..., PartitionFilters: [], PushedFilters: [IsNotNull(qty), GreaterThan(qty,10), EqualTo(name,banana), IsNotNull(name)], ReadSchema: struct<name:string,qty:int,date:string><br><br>=============================================================<br>Plan without indexes:<br>=============================================================<br>SortMergeJoin [name#1408], [name#1414], Inner<br>:- *(2) Sort [name#1408 ASC NULLS FIRST], false, 0<br>:  +- Exchange hashpartitioning(name#1408, 200), [id=#634]<br>:     +- *(1) Project [name#1408, qty#1409, date#1410]<br>:        +- *(1) Filter (isnotnull(name#1408) && (name#1408 = banana))<br>:           +- *(1) FileScan parquet [name#1408,qty#1409,date#1410] Batched: true, Format: Parquet, Location: InMemoryFileIndex[abfss://default@honghaigen2.dfs.core.windows.net/hyperspace/data-6785343/produc..., PartitionFilters: [], PushedFilters: [IsNotNull(name), EqualTo(name,banana)], ReadSchema: struct<name:string,qty:int,date:string><br>+- *(4) Sort [name#1414 ASC NULLS FIRST], false, 0<br>   +- Exchange hashpartitioning(name#1414, 200), [id=#640]<br>      +- *(3) Project [name#1414, qty#1415, date#1416]<br>         +- *(3) Filter (((isnotnull(qty#1415) && (qty#1415 > 10)) && (name#1414 = banana)) && isnotnull(name#1414))<br>            +- *(3) FileScan parquet [name#1414,qty#1415,date#1416] Batched: true, Format: Parquet, Location: InMemoryFileIndex[abfss://default@honghaigen2.dfs.core.windows.net/hyperspace/data-6785343/produc..., PartitionFilters: [], PushedFilters: [IsNotNull(qty), GreaterThan(qty,10), EqualTo(name,banana), IsNotNull(name)], ReadSchema: struct<name:string,qty:int,date:string><br><br>=============================================================<br>Indexes used:<br>=============================================================<br><br>=============================================================<br>Physical operator stats:<br>=============================================================<br>+-----------------+-------------------+------------------+----------+<br>|Physical Operator|Hyperspace Disabled|Hyperspace Enabled|Difference|<br>+-----------------+-------------------+------------------+----------+<br>|           Filter|                  2|                 2|         0|<br>|     InputAdapter|                  4|                 4|         0|<br>|          Project|                  2|                 2|         0|<br>|     Scan parquet|                  2|                 2|         0|<br>|  ShuffleExchange|                  2|                 2|         0|<br>|             Sort|                  2|                 2|         0|<br>|    SortMergeJoin|                  1|                 1|         0|<br>|WholeStageCodegen|                  5|                 5|         0|<br>+-----------------+-------------------+------------------+----------+<br><br></pre>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "\n"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 25,
          "data": {},
          "metadata": {}
        }
      ],
      "execution_count": 25,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Enable Hybrid Scan\n",
        "\n",
        "In plan with indexes, you can see\n",
        "`Exchange hashpartitioning` required only for appended files so that we could still utilize the \"shuffled\" index data with appended files. `BucketUnion` is used to merge \"shuffled\" appended files with the index data."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "// Enable Hybrid Scan config. \"delete\" config is not necessary.\n",
        "spark.Conf().Set(\"spark.hyperspace.index.hybridscan.enabled\", \"true\");\n",
        "// spark.Conf().Set(\"spark.hyperspace.index.hybridscan.delete.enabled\", \"true\");\n",
        "spark.EnableHyperspace();\n",
        "// Need to redefine query to recalculate the query plan.\n",
        "DataFrame query = filter1.Join(filter2, filter1.Col(\"name\") == filter2.Col(\"name\"));\n",
        "\n",
        "query.Show();\n",
        "\n",
        "hyperspace.Explain(query, true, input => DisplayHTML(input));"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "medium",
              "session_id": 21,
              "statement_id": 26,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-03-15T07:01:48.2325384Z",
              "execution_start_time": "2021-03-15T07:03:30.7688162Z",
              "execution_finish_time": "2021-03-15T07:03:32.8213055Z"
            },
            "text/plain": "StatementMeta(medium, 21, 26, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "+------+---+----------+------+---+----------+\n|  name|qty|      date|  name|qty|      date|\n+------+---+----------+------+---+----------+\n|banana| 11|2020-10-03|banana| 11|2020-10-03|\n|banana|  1|2020-10-01|banana| 11|2020-10-03|\n|banana|  5|2020-11-01|banana| 11|2020-10-03|\n+------+---+----------+------+---+----------+\n\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": "<pre>=============================================================<br>Plan with indexes:<br>=============================================================<br>SortMergeJoin [name#1408], [name#1564], Inner<br>:- *(3) Sort [name#1408 ASC NULLS FIRST], false, 0<br><b style=\"background:LightGreen\">:  +- BucketUnion 200 buckets, bucket columns: [name]</b><br><b style=\"background:LightGreen\">:     :- *(1) Project [name#1408, qty#1409, date#1410]</b><br><b style=\"background:LightGreen\">:     :  +- *(1) Filter (isnotnull(name#1408) && (name#1408 = banana))</b><br><b style=\"background:LightGreen\">:     :     +- *(1) FileScan Hyperspace(Type: CI, Name: productIndex, LogVersion: 1) [name#1408,date#1410,qty#1409] Batched: true, Format: Parquet, Location: InMemoryFileIndex[abfss://default@honghaigen2.dfs.core.windows.net/hyperspace/indexes-6785343/pro..., PartitionFilters: [], PushedFilters: [IsNotNull(name), EqualTo(name,banana)], ReadSchema: struct<name:string,date:string,qty:int>, SelectedBucketsCount: 1 out of 200</b><br><b style=\"background:LightGreen\">:     +- Exchange hashpartitioning(name#1408, 200), [id=#886]</b><br><b style=\"background:LightGreen\">:        +- *(2) Project [name#1408, qty#1409, date#1410]</b><br><b style=\"background:LightGreen\">:           +- *(2) Filter (isnotnull(name#1408) && (name#1408 = banana))</b><br><b style=\"background:LightGreen\">:              +- *(2) FileScan parquet [name#1408,qty#1409,date#1410] Batched: true, Format: Parquet, Location: InMemoryFileIndex[abfss://default@honghaigen2.dfs.core.windows.net/hyperspace/data-6785343/produc..., PartitionFilters: [], PushedFilters: [IsNotNull(name), EqualTo(name,banana)], ReadSchema: struct<name:string,qty:int,date:string></b><br>+- *(6) Sort [name#1564 ASC NULLS FIRST], false, 0<br>   <b style=\"background:LightGreen\">+- BucketUnion 200 buckets, bucket columns: [name]</b><br>      <b style=\"background:LightGreen\">:- *(4) Project [name#1564, qty#1565, date#1566]</b><br>      <b style=\"background:LightGreen\">:  +- *(4) Filter (((isnotnull(qty#1565) && (qty#1565 > 10)) && isnotnull(name#1564)) && (name#1564 = banana))</b><br>      <b style=\"background:LightGreen\">:     +- *(4) FileScan Hyperspace(Type: CI, Name: productIndex, LogVersion: 1) [name#1564,date#1566,qty#1565] Batched: true, Format: Parquet, Location: InMemoryFileIndex[abfss://default@honghaigen2.dfs.core.windows.net/hyperspace/indexes-6785343/pro..., PartitionFilters: [], PushedFilters: [IsNotNull(qty), GreaterThan(qty,10), IsNotNull(name), EqualTo(name,banana)], ReadSchema: struct<name:string,date:string,qty:int>, SelectedBucketsCount: 1 out of 200</b><br>      <b style=\"background:LightGreen\">+- Exchange hashpartitioning(name#1564, 200), [id=#894]</b><br>         <b style=\"background:LightGreen\">+- *(5) Project [name#1564, qty#1565, date#1566]</b><br>            <b style=\"background:LightGreen\">+- *(5) Filter (((isnotnull(qty#1565) && (qty#1565 > 10)) && isnotnull(name#1564)) && (name#1564 = banana))</b><br>               <b style=\"background:LightGreen\">+- *(5) FileScan parquet [name#1564,qty#1565,date#1566] Batched: true, Format: Parquet, Location: InMemoryFileIndex[abfss://default@honghaigen2.dfs.core.windows.net/hyperspace/data-6785343/produc..., PartitionFilters: [], PushedFilters: [IsNotNull(qty), GreaterThan(qty,10), IsNotNull(name), EqualTo(name,banana)], ReadSchema: struct<name:string,qty:int,date:string></b><br><br>=============================================================<br>Plan without indexes:<br>=============================================================<br>SortMergeJoin [name#1408], [name#1564], Inner<br>:- *(2) Sort [name#1408 ASC NULLS FIRST], false, 0<br><b style=\"background:LightGreen\">:  +- Exchange hashpartitioning(name#1408, 200), [id=#817]</b><br><b style=\"background:LightGreen\">:     +- *(1) Project [name#1408, qty#1409, date#1410]</b><br><b style=\"background:LightGreen\">:        +- *(1) Filter (isnotnull(name#1408) && (name#1408 = banana))</b><br><b style=\"background:LightGreen\">:           +- *(1) FileScan parquet [name#1408,qty#1409,date#1410] Batched: true, Format: Parquet, Location: InMemoryFileIndex[abfss://default@honghaigen2.dfs.core.windows.net/hyperspace/data-6785343/produc..., PartitionFilters: [], PushedFilters: [IsNotNull(name), EqualTo(name,banana)], ReadSchema: struct<name:string,qty:int,date:string></b><br>+- *(4) Sort [name#1564 ASC NULLS FIRST], false, 0<br>   <b style=\"background:LightGreen\">+- Exchange hashpartitioning(name#1564, 200), [id=#823]</b><br>      <b style=\"background:LightGreen\">+- *(3) Project [name#1564, qty#1565, date#1566]</b><br>         <b style=\"background:LightGreen\">+- *(3) Filter (((isnotnull(qty#1565) && (qty#1565 > 10)) && isnotnull(name#1564)) && (name#1564 = banana))</b><br>            <b style=\"background:LightGreen\">+- *(3) FileScan parquet [name#1564,qty#1565,date#1566] Batched: true, Format: Parquet, Location: InMemoryFileIndex[abfss://default@honghaigen2.dfs.core.windows.net/hyperspace/data-6785343/produc..., PartitionFilters: [], PushedFilters: [IsNotNull(qty), GreaterThan(qty,10), IsNotNull(name), EqualTo(name,banana)], ReadSchema: struct<name:string,qty:int,date:string></b><br><br>=============================================================<br>Indexes used:<br>=============================================================<br>productIndex:abfss://default@honghaigen2.dfs.core.windows.net/hyperspace/indexes-6785343/productIndex/v__=0<br><br>=============================================================<br>Physical operator stats:<br>=============================================================<br>+-------------------------------------------------------------+-------------------+------------------+----------+<br>|                                            Physical Operator|Hyperspace Disabled|Hyperspace Enabled|Difference|<br>+-------------------------------------------------------------+-------------------+------------------+----------+<br>|                                                 *BucketUnion|                  0|                 2|         2|<br>|                                                      *Filter|                  2|                 4|         2|<br>|                                                     *Project|                  2|                 4|         2|<br>|*Scan Hyperspace(Type: CI, Name: productIndex, LogVersion: 1)|                  0|                 2|         2|<br>|                                           *WholeStageCodegen|                  5|                 7|         2|<br>|                                                 InputAdapter|                  4|                 4|         0|<br>|                                                 Scan parquet|                  2|                 2|         0|<br>|                                              ShuffleExchange|                  2|                 2|         0|<br>|                                                         Sort|                  2|                 2|         0|<br>|                                                SortMergeJoin|                  1|                 1|         0|<br>+-------------------------------------------------------------+-------------------+------------------+----------+<br><br></pre>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "\n"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 26,
          "data": {},
          "metadata": {}
        }
      ],
      "execution_count": 26,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cleanup\n",
        "To make this notebook self-contained and not leave any dangling data, we have some small clean-up code below. "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "using Microsoft.Spark.Extensions.Azure.Synapse.Analytics.Notebook.MSSparkUtils;\n",
        "\n",
        "FS.Rm(dataPath, true);\n",
        "FS.Rm(indexLocation, true);"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "medium",
              "session_id": 21,
              "statement_id": 27,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2021-03-15T07:01:49.2435441Z",
              "execution_start_time": "2021-03-15T07:03:32.9118848Z",
              "execution_finish_time": "2021-03-15T07:03:34.986281Z"
            },
            "text/plain": "StatementMeta(medium, 21, 27, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 27,
          "data": {
            "text/plain": ""
          },
          "metadata": {}
        }
      ],
      "execution_count": 27,
      "metadata": {}
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_sparkdotnet",
      "language": "C#",
      "display_name": "Synapse SparkDotNet"
    },
    "language_info": {
      "name": "csharp"
    },
    "microsoft": {
      "language": "csharp"
    },
    "kernel_info": {
      "name": "synapse_sparkdotnet"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}