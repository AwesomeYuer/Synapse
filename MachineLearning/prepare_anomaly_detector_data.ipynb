{
  "metadata": {
    "saveOutput": true,
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "from pyspark.sql.functions import lit\n",
        "\n",
        "df = spark.createDataFrame([\n",
        "    (\"1972-01-01T00:00:00Z\", 826.0),\n",
        "    (\"1972-02-01T00:00:00Z\", 799.0),\n",
        "    (\"1972-03-01T00:00:00Z\", 890.0),\n",
        "    (\"1972-04-01T00:00:00Z\", 900.0),\n",
        "    (\"1972-05-01T00:00:00Z\", 766.0),\n",
        "    (\"1972-06-01T00:00:00Z\", 805.0),\n",
        "    (\"1972-07-01T00:00:00Z\", 821.0),\n",
        "    (\"1972-08-01T00:00:00Z\", 20000.0),\n",
        "    (\"1972-09-01T00:00:00Z\", 883.0),\n",
        "    (\"1972-10-01T00:00:00Z\", 898.0),\n",
        "    (\"1972-11-01T00:00:00Z\", 957.0),\n",
        "    (\"1972-12-01T00:00:00Z\", 924.0),\n",
        "    (\"1973-01-01T00:00:00Z\", 881.0),\n",
        "    (\"1973-02-01T00:00:00Z\", 837.0),\n",
        "    (\"1973-03-01T00:00:00Z\", 9000.0)\n",
        "], [\"timestamp\", \"value\"]).withColumn(\"group\", lit(\"series1\"))"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "df.write.mode(\"overwrite\").saveAsTable(\"anomaly_detector_testing_data\")"
      ],
      "attachments": {}
    }
  ]
}